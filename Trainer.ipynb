{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "138c0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, roc_curve, auc, accuracy_score,precision_recall_curve, f1_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8b49325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiLSTMSequenceProcessor:\n",
    "    \"\"\"Process raw packet data into sequences for BiLSTM\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=50):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def create_sequences_from_pcap(self, pcap_file, label, max_flows=1000, packets_per_flow=50):\n",
    "        \"\"\"\n",
    "        Create sequences directly from PCAP file\n",
    "        Each sequence = packets from a single flow\n",
    "        \"\"\"\n",
    "        from scapy.all import PcapReader, IP, TCP, UDP\n",
    "        \n",
    "        print(f\"\\nüì¶ Processing {pcap_file} into sequences...\")\n",
    "        \n",
    "        flow_dict = {}\n",
    "        sequences = []\n",
    "        labels = []\n",
    "        \n",
    "        try:\n",
    "            with PcapReader(pcap_file) as pcap:\n",
    "                for pkt_idx, packet in enumerate(tqdm(pcap, desc=\"   Reading packets\")):\n",
    "                    if IP not in packet:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract flow identifier\n",
    "                    src_ip = packet[IP].src\n",
    "                    dst_ip = packet[IP].dst\n",
    "                    proto = packet[IP].proto\n",
    "                    \n",
    "                    if TCP in packet:\n",
    "                        src_port = packet[TCP].sport\n",
    "                        dst_port = packet[TCP].dport\n",
    "                    elif UDP in packet:\n",
    "                        src_port = packet[UDP].sport\n",
    "                        dst_port = packet[UDP].dport\n",
    "                    else:\n",
    "                        src_port = 0\n",
    "                        dst_port = 0\n",
    "                    \n",
    "                    # Create flow key (bidirectional)\n",
    "                    flow_key = tuple(sorted([\n",
    "                        (src_ip, src_port, proto),\n",
    "                        (dst_ip, dst_port, proto)\n",
    "                    ]))\n",
    "                    \n",
    "                    # Extract packet features for sequence\n",
    "                    features = self._extract_sequence_features(packet)\n",
    "                    \n",
    "                    if flow_key not in flow_dict:\n",
    "                        flow_dict[flow_key] = []\n",
    "                    \n",
    "                    flow_dict[flow_key].append(features)\n",
    "            \n",
    "            # Convert flows to sequences\n",
    "            print(f\"\\n   Converting {len(flow_dict)} flows to sequences...\")\n",
    "            for flow_key, packets in tqdm(flow_dict.items(), desc=\"   Creating sequences\"):\n",
    "                if len(packets) >= 3:  # Minimum 3 packets per flow\n",
    "                    # Pad or truncate to sequence_length\n",
    "                    if len(packets) < self.sequence_length:\n",
    "                        # Pad with zeros\n",
    "                        packets = packets + [[0]*13] * (self.sequence_length - len(packets))\n",
    "                    else:\n",
    "                        # Take first sequence_length packets\n",
    "                        packets = packets[:self.sequence_length]\n",
    "                    \n",
    "                    sequences.append(np.array(packets))\n",
    "                    labels.append(label)\n",
    "                    \n",
    "                    if len(sequences) >= max_flows:\n",
    "                        break\n",
    "            \n",
    "            print(f\"   ‚úì Created {len(sequences)} sequences\")\n",
    "            return np.array(sequences), np.array(labels)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "            return np.array([]), np.array([])\n",
    "    \n",
    "    def _extract_sequence_features(self, packet):\n",
    "        \"\"\"Extract 13 key features from a packet\"\"\"\n",
    "        from scapy.all import IP, TCP, UDP, ICMP, DNS\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        try:\n",
    "            if IP in packet:\n",
    "                # Packet length\n",
    "                features.append(len(packet) / 1500.0)  # Normalize\n",
    "                \n",
    "                # IP TTL\n",
    "                features.append(packet[IP].ttl / 255.0)\n",
    "                \n",
    "                # IP flags\n",
    "                features.append(int(packet[IP].flags) / 7.0)\n",
    "                \n",
    "                # IP ID (variation indicator)\n",
    "                features.append((packet[IP].id % 1000) / 1000.0)\n",
    "                \n",
    "                if TCP in packet:\n",
    "                    # TCP sport/dport ratio\n",
    "                    features.append(packet[TCP].sport / 65535.0)\n",
    "                    features.append(packet[TCP].dport / 65535.0)\n",
    "                    \n",
    "                    # TCP flags\n",
    "                    features.append(int(packet[TCP].flags) / 63.0)\n",
    "                    \n",
    "                    # TCP window size\n",
    "                    features.append(packet[TCP].window / 65535.0)\n",
    "                    \n",
    "                    # Payload length\n",
    "                    payload_len = len(packet[TCP].payload)\n",
    "                    features.append(payload_len / 1500.0)\n",
    "                    \n",
    "                    # Has options\n",
    "                    features.append(float(len(packet[TCP].options)))\n",
    "                    \n",
    "                    # Entropy indicator\n",
    "                    features.append(self._payload_entropy(bytes(packet[TCP].payload)))\n",
    "                    \n",
    "                    features.extend([0.0, 0.0])  # Padding for UDP\n",
    "                    \n",
    "                elif UDP in packet:\n",
    "                    # UDP sport/dport\n",
    "                    features.append(packet[UDP].sport / 65535.0)\n",
    "                    features.append(packet[UDP].dport / 65535.0)\n",
    "                    \n",
    "                    # Payload length\n",
    "                    payload_len = len(packet[UDP].payload)\n",
    "                    features.append(payload_len / 1500.0)\n",
    "                    \n",
    "                    features.extend([0.0, 0.0, 0.0, 0.0, 0.0])  # Padding for TCP\n",
    "                    \n",
    "                else:\n",
    "                    features.extend([0.0] * 10)\n",
    "                \n",
    "                # ICMP check\n",
    "                if ICMP in packet:\n",
    "                    features.append(1.0)\n",
    "                else:\n",
    "                    features.append(0.0)\n",
    "                \n",
    "                # DNS check\n",
    "                if DNS in packet:\n",
    "                    features.append(1.0)\n",
    "                else:\n",
    "                    features.append(0.0)\n",
    "            else:\n",
    "                features = [0.0] * 13\n",
    "        \n",
    "        except:\n",
    "            features = [0.0] * 13\n",
    "        \n",
    "        return features[:13]  # Ensure exactly 13 features\n",
    "    \n",
    "    def _payload_entropy(self, payload):\n",
    "        \"\"\"Calculate normalized Shannon entropy of payload\"\"\"\n",
    "        if not payload or len(payload) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        from scipy.stats import entropy\n",
    "        byte_counts = np.bincount(list(payload), minlength=256)\n",
    "        probabilities = byte_counts[byte_counts > 0] / len(payload)\n",
    "        ent = entropy(probabilities, base=2) / 8.0  # Normalize to 0-1\n",
    "        return min(ent, 1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbc9425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier:\n",
    "    \"\"\"BiLSTM classifier for steganography detection\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.scaler_x = StandardScaler()\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        \n",
    "    def load_sequences(self, benign_pcap, stego_pcap, max_flows_per_class=1000, \n",
    "                       sequence_length=50):\n",
    "        \"\"\"Load sequences from PCAP files\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" LOADING PACKET SEQUENCES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        processor = BiLSTMSequenceProcessor(sequence_length=sequence_length)\n",
    "        \n",
    "        # Process benign traffic\n",
    "        X_benign, y_benign = processor.create_sequences_from_pcap(\n",
    "            benign_pcap, label=0, max_flows=max_flows_per_class\n",
    "        )\n",
    "        \n",
    "        # Process steganographic traffic\n",
    "        X_stego, y_stego = processor.create_sequences_from_pcap(\n",
    "            stego_pcap, label=1, max_flows=max_flows_per_class\n",
    "        )\n",
    "        \n",
    "        # Combine datasets\n",
    "        X = np.concatenate([X_benign, X_stego], axis=0)\n",
    "        y = np.concatenate([y_benign, y_stego], axis=0)\n",
    "        \n",
    "        print(f\"\\n‚úì Total sequences: {len(X)}\")\n",
    "        print(f\"  - Benign: {len(X_benign)}\")\n",
    "        print(f\"  - Steganography: {len(X_stego)}\")\n",
    "        print(f\"  - Sequence length: {sequence_length}\")\n",
    "        print(f\"  - Features per packet: 13\")\n",
    "        \n",
    "        return X, y, processor\n",
    "    \n",
    "    def preprocess_sequences(self, X, y, test_size=0.2, random_state=42):\n",
    "        \"\"\"Preprocess sequences\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" PREPROCESSING SEQUENCES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nüìä Splitting sequences...\")\n",
    "        \n",
    "        # Train-test split\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"   Train: {len(self.X_train)} sequences\")\n",
    "        print(f\"   Test:  {len(self.X_test)} sequences\")\n",
    "        \n",
    "        # Normalize sequences (per timestep)\n",
    "        print(f\"\\nüîß Normalizing sequences...\")\n",
    "        seq_len, n_features = self.X_train[0].shape\n",
    "        \n",
    "        # Reshape for scaling\n",
    "        X_train_reshaped = self.X_train.reshape(-1, n_features)\n",
    "        X_test_reshaped = self.X_test.reshape(-1, n_features)\n",
    "        \n",
    "        X_train_scaled = self.scaler_x.fit_transform(X_train_reshaped)\n",
    "        X_test_scaled = self.scaler_x.transform(X_test_reshaped)\n",
    "        \n",
    "        self.X_train = X_train_scaled.reshape(self.X_train.shape)\n",
    "        self.X_test = X_test_scaled.reshape(self.X_test.shape)\n",
    "        \n",
    "        print(f\"   ‚úì Sequences normalized\")\n",
    "        print(f\"   Shape: {self.X_train.shape}\")\n",
    "    \n",
    "    def build_bilstm_model(self, sequence_length=50, n_features=13):\n",
    "        \"\"\"Build BiLSTM model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" BUILDING BiLSTM MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nüèóÔ∏è  Architecture:\")\n",
    "        print(f\"   Input shape: ({sequence_length}, {n_features})\")\n",
    "        \n",
    "        self.model = models.Sequential([\n",
    "            # BiLSTM layers\n",
    "            layers.Input(shape=(sequence_length, n_features)),\n",
    "            \n",
    "            # First BiLSTM block\n",
    "            layers.Bidirectional(\n",
    "                layers.LSTM(128, return_sequences=True, dropout=0.2),\n",
    "                name='bilstm_1'\n",
    "            ),\n",
    "            layers.Dropout(0.3, name='dropout_1'),\n",
    "            \n",
    "            # Second BiLSTM block\n",
    "            layers.Bidirectional(\n",
    "                layers.LSTM(64, return_sequences=True, dropout=0.2),\n",
    "                name='bilstm_2'\n",
    "            ),\n",
    "            layers.Dropout(0.2, name='dropout_2'),\n",
    "            \n",
    "            # Third BiLSTM block\n",
    "            layers.Bidirectional(\n",
    "                layers.LSTM(32, return_sequences=False, dropout=0.2),\n",
    "                name='bilstm_3'\n",
    "            ),\n",
    "            layers.Dropout(0.2, name='dropout_3'),\n",
    "            \n",
    "            # Dense layers\n",
    "            layers.Dense(64, activation='relu', name='dense_1'),\n",
    "            layers.BatchNormalization(name='bn_1'),\n",
    "            layers.Dropout(0.2, name='dropout_4'),\n",
    "            \n",
    "            layers.Dense(32, activation='relu', name='dense_2'),\n",
    "            layers.Dropout(0.1, name='dropout_5'),\n",
    "            \n",
    "            # Output\n",
    "            layers.Dense(1, activation='sigmoid', name='output')\n",
    "        ])\n",
    "        \n",
    "        print(\"   Layers:\")\n",
    "        print(\"   ‚îú‚îÄ BiLSTM: 128 (return_seq=True, dropout=0.2) ‚Üí Dropout(0.3)\")\n",
    "        print(\"   ‚îú‚îÄ BiLSTM: 64 (return_seq=True, dropout=0.2) ‚Üí Dropout(0.2)\")\n",
    "        print(\"   ‚îú‚îÄ BiLSTM: 32 (return_seq=False, dropout=0.2) ‚Üí Dropout(0.2)\")\n",
    "        print(\"   ‚îú‚îÄ Dense: 64 ‚Üí ReLU ‚Üí BatchNorm ‚Üí Dropout(0.2)\")\n",
    "        print(\"   ‚îú‚îÄ Dense: 32 ‚Üí ReLU ‚Üí Dropout(0.1)\")\n",
    "        print(\"   ‚îî‚îÄ Output: 1 ‚Üí Sigmoid\")\n",
    "        \n",
    "        # Compile\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                keras.metrics.AUC(name='auc'),\n",
    "                keras.metrics.Precision(name='precision'),\n",
    "                keras.metrics.Recall(name='recall')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úì Model compiled\")\n",
    "        print(\"   Optimizer: Adam (lr=0.001)\")\n",
    "        print(\"   Loss: Binary Crossentropy\")\n",
    "    \n",
    "    def train_model(self, epochs=100, batch_size=32, validation_split=0.2):\n",
    "        \"\"\"Train BiLSTM model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" TRAINING BiLSTM MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  Parameters:\")\n",
    "        print(f\"   Epochs: {epochs}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Validation split: {validation_split*100:.0f}%\")\n",
    "        \n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=20,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=10,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training...\\n\")\n",
    "        \n",
    "        self.history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úì Training completed!\")\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate on test set\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" MODEL EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìä Generating predictions...\")\n",
    "        y_pred_proba = self.model.predict(self.X_test, verbose=0)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        auc_score = roc_auc_score(self.y_test, y_pred_proba)\n",
    "        f1 = f1_score(self.y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\n‚úì Test Metrics:\")\n",
    "        print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"   AUC-ROC:   {auc_score:.4f}\")\n",
    "        print(f\"   F1-Score:  {f1:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìã Classification Report:\")\n",
    "        print(classification_report(\n",
    "            self.y_test, y_pred,\n",
    "            target_names=['Benign', 'Steganography']\n",
    "        ))\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        print(f\"\\nüîç Confusion Matrix:\")\n",
    "        print(f\"   True Negatives:  {cm[0,0]:>6,}\")\n",
    "        print(f\"   False Positives: {cm[0,1]:>6,}\")\n",
    "        print(f\"   False Negatives: {cm[1,0]:>6,}\")\n",
    "        print(f\"   True Positives:  {cm[1,1]:>6,}\")\n",
    "        \n",
    "        return y_pred, y_pred_proba, cm\n",
    "    \n",
    "    def plot_results(self, y_pred, y_pred_proba, cm, output_dir='results'):\n",
    "        \"\"\"Plot training and evaluation results\"\"\"\n",
    "        print(\"\\nüìà Generating visualizations...\")\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Training history\n",
    "        ax1 = plt.subplot(2, 3, 1)\n",
    "        ax1.plot(self.history.history['accuracy'], label='Train', linewidth=2)\n",
    "        ax1.plot(self.history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "        ax1.set_title('Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2 = plt.subplot(2, 3, 2)\n",
    "        ax2.plot(self.history.history['loss'], label='Train', linewidth=2)\n",
    "        ax2.plot(self.history.history['val_loss'], label='Validation', linewidth=2)\n",
    "        ax2.set_title('Loss', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        ax3.plot(self.history.history['auc'], label='Train', linewidth=2)\n",
    "        ax3.plot(self.history.history['val_auc'], label='Validation', linewidth=2)\n",
    "        ax3.set_title('AUC-ROC', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('AUC')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax4,\n",
    "                    xticklabels=['Benign', 'Stego'],\n",
    "                    yticklabels=['Benign', 'Stego'])\n",
    "        ax4.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # ROC curve\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax5.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.4f})', linewidth=2)\n",
    "        ax5.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "        ax5.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "        ax5.set_xlabel('False Positive Rate')\n",
    "        ax5.set_ylabel('True Positive Rate')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # PR curve\n",
    "        ax6 = plt.subplot(2, 3, 6)\n",
    "        precision, recall, _ = precision_recall_curve(self.y_test, y_pred_proba)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        ax6.plot(recall, precision, label=f'PR (AUC = {pr_auc:.4f})', linewidth=2)\n",
    "        ax6.set_title('Precision-Recall', fontsize=12, fontweight='bold')\n",
    "        ax6.set_xlabel('Recall')\n",
    "        ax6.set_ylabel('Precision')\n",
    "        ax6.legend()\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = os.path.join(output_dir, 'bilstm_results.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úì Saved to: {plot_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Add this method to the BiLSTMClassifier class in train_bilstm.py\n",
    "\n",
    "    def save_model(self, output_dir='models'):\n",
    "        \"\"\"Save model, scaler, and metadata\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" SAVING MODEL AND ARTIFACTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(output_dir, f'bilstm_stego_{timestamp}.h5')\n",
    "        self.model.save(model_path)\n",
    "        print(f\"\\n‚úì Model saved: {model_path}\")\n",
    "        \n",
    "        # Save scaler (IMPORTANT!)\n",
    "        scaler_path = os.path.join(output_dir, f'scaler_{timestamp}.pkl')\n",
    "        import pickle\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(self.scaler_x, f)\n",
    "        print(f\"‚úì Scaler saved: {scaler_path}\")\n",
    "        \n",
    "        # Save model summary\n",
    "        summary_path = os.path.join(output_dir, f'model_summary_{timestamp}.txt')\n",
    "        with open(summary_path, 'w') as f:\n",
    "            self.model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        print(f\"‚úì Model summary saved: {summary_path}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'timestamp': timestamp,\n",
    "            'sequence_length': 50,\n",
    "            'n_features': 13,\n",
    "            'model_path': model_path,\n",
    "            'scaler_path': scaler_path,\n",
    "            'training_date': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(output_dir, f'metadata_{timestamp}.pkl')\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        print(f\"‚úì Metadata saved: {metadata_path}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" FILES SAVED IN ./models/\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nüìÅ Files created:\")\n",
    "        print(f\"   1. {os.path.basename(model_path)}\")\n",
    "        print(f\"   2. {os.path.basename(scaler_path)}\")\n",
    "        print(f\"   3. {os.path.basename(summary_path)}\")\n",
    "        print(f\"   4. {os.path.basename(metadata_path)}\")\n",
    "        print(f\"\\nüí° Use these for inference:\")\n",
    "        print(f\"   model_path = '{model_path}'\")\n",
    "        print(f\"   scaler_path = '{scaler_path}'\")\n",
    "        print(\"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b31564d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " LOADING PACKET SEQUENCES\n",
      "======================================================================\n",
      "\n",
      "üì¶ Processing output_86k.pcap into sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Reading packets: 86000it [00:42, 2010.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Converting 38444 flows to sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Creating sequences:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 18752/38444 [00:00<00:00, 276952.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Created 500 sequences\n",
      "\n",
      "üì¶ Processing steganography_dataset_20251016_020211.pcap into sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Reading packets: 92067it [01:15, 1222.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Converting 81964 flows to sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Creating sequences:  14%|‚ñà‚ñé        | 11101/81964 [00:00<00:00, 177748.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Created 500 sequences\n",
      "\n",
      "‚úì Total sequences: 1000\n",
      "  - Benign: 500\n",
      "  - Steganography: 500\n",
      "  - Sequence length: 50\n",
      "  - Features per packet: 13\n",
      "\n",
      "======================================================================\n",
      " PREPROCESSING SEQUENCES\n",
      "======================================================================\n",
      "\n",
      "üìä Splitting sequences...\n",
      "   Train: 800 sequences\n",
      "   Test:  200 sequences\n",
      "\n",
      "üîß Normalizing sequences...\n",
      "   ‚úì Sequences normalized\n",
      "   Shape: (800, 50, 13)\n",
      "\n",
      "======================================================================\n",
      " BUILDING BiLSTM MODEL\n",
      "======================================================================\n",
      "\n",
      "üèóÔ∏è  Architecture:\n",
      "   Input shape: (50, 13)\n",
      "   Layers:\n",
      "   ‚îú‚îÄ BiLSTM: 128 (return_seq=True, dropout=0.2) ‚Üí Dropout(0.3)\n",
      "   ‚îú‚îÄ BiLSTM: 64 (return_seq=True, dropout=0.2) ‚Üí Dropout(0.2)\n",
      "   ‚îú‚îÄ BiLSTM: 32 (return_seq=False, dropout=0.2) ‚Üí Dropout(0.2)\n",
      "   ‚îú‚îÄ Dense: 64 ‚Üí ReLU ‚Üí BatchNorm ‚Üí Dropout(0.2)\n",
      "   ‚îú‚îÄ Dense: 32 ‚Üí ReLU ‚Üí Dropout(0.1)\n",
      "   ‚îî‚îÄ Output: 1 ‚Üí Sigmoid\n",
      "\n",
      "‚úì Model compiled\n",
      "   Optimizer: Adam (lr=0.001)\n",
      "   Loss: Binary Crossentropy\n",
      "\n",
      "======================================================================\n",
      " TRAINING BiLSTM MODEL\n",
      "======================================================================\n",
      "\n",
      "‚öôÔ∏è  Parameters:\n",
      "   Epochs: 10\n",
      "   Batch size: 32\n",
      "   Validation split: 20%\n",
      "\n",
      "üöÄ Starting training...\n",
      "\n",
      "Epoch 1/10\n",
      "20/20 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3:47 12s/step - accuracy: 0.4062 - auc: 0.4147 - loss: 0.8762 - precision: 0.1429 - recall: 0.071 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 81ms/step - accuracy: 0.4766 - auc: 0.5058 - loss: 0.7829 - precision: 0.2589 - recall: 0.155 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 73ms/step - accuracy: 0.5122 - auc: 0.5651 - loss: 0.7443 - precision: 0.3726 - recall: 0.23 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 71ms/step - accuracy: 0.5501 - auc: 0.6127 - loss: 0.7078 - precision: 0.4556 - recall: 0.30 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 68ms/step - accuracy: 0.5814 - auc: 0.6520 - loss: 0.6733 - precision: 0.5087 - recall: 0.36 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.6077 - auc: 0.6838 - loss: 0.6448 - precision: 0.5510 - recall: 0.41 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.6312 - auc: 0.7106 - loss: 0.6186 - precision: 0.5854 - recall: 0.45 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.6505 - auc: 0.7319 - loss: 0.5970 - precision: 0.6125 - recall: 0.49 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.6677 - auc: 0.7502 - loss: 0.5778 - precision: 0.6355 - recall: 0.52 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.6828 - auc: 0.7662 - loss: 0.5599 - precision: 0.6543 - recall: 0.54 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.6964 - auc: 0.7803 - loss: 0.5436 - precision: 0.6712 - recall: 0.57 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.7089 - auc: 0.7930 - loss: 0.5283 - precision: 0.6865 - recall: 0.59 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.7200 - auc: 0.8043 - loss: 0.5141 - precision: 0.6996 - recall: 0.61 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.7301 - auc: 0.8144 - loss: 0.5009 - precision: 0.7113 - recall: 0.63 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.7392 - auc: 0.8234 - loss: 0.4887 - precision: 0.7215 - recall: 0.64 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.7472 - auc: 0.8314 - loss: 0.4775 - precision: 0.7300 - recall: 0.66 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.7547 - auc: 0.8387 - loss: 0.4670 - precision: 0.7380 - recall: 0.67 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.7618 - auc: 0.8455 - loss: 0.4570 - precision: 0.7456 - recall: 0.68 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.7685 - auc: 0.8517 - loss: 0.4476 - precision: 0.7529 - recall: 0.69 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.7746 - auc: 0.8573 - loss: 0.4389 - precision: 0.7594 - recall: 0.70 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 15s 175ms/step - accuracy: 0.8906 - auc: 0.9645 - loss: 0.2718 - precision: 0.8824 - recall: 0.8991 - val_accuracy: 0.9875 - val_auc: 1.0000 - val_loss: 0.3630 - val_precision: 0.9765 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "20/20 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 102ms/step - accuracy: 0.9688 - auc: 1.0000 - loss: 0.0738 - precision: 0.9231 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 68ms/step - accuracy: 0.9766 - auc: 1.0000 - loss: 0.0717 - precision: 0.9437 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 66ms/step - accuracy: 0.9774 - auc: 0.9993 - loss: 0.0782 - precision: 0.9470 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 65ms/step - accuracy: 0.9772 - auc: 0.9991 - loss: 0.0815 - precision: 0.9471 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9780 - auc: 0.9990 - loss: 0.0813 - precision: 0.9494 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 64ms/step - accuracy: 0.9791 - auc: 0.9989 - loss: 0.0808 - precision: 0.9524 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 64ms/step - accuracy: 0.9795 - auc: 0.9988 - loss: 0.0805 - precision: 0.9552 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9801 - auc: 0.9987 - loss: 0.0798 - precision: 0.9578 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9808 - auc: 0.9987 - loss: 0.0791 - precision: 0.9601 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9815 - auc: 0.9986 - loss: 0.0783 - precision: 0.9622 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9821 - auc: 0.9986 - loss: 0.0774 - precision: 0.9641 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9827 - auc: 0.9987 - loss: 0.0763 - precision: 0.9657 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9831 - auc: 0.9987 - loss: 0.0754 - precision: 0.9673 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9834 - auc: 0.9987 - loss: 0.0746 - precision: 0.9686 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9834 - auc: 0.9987 - loss: 0.0740 - precision: 0.9693 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9833 - auc: 0.9987 - loss: 0.0735 - precision: 0.9701 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9833 - auc: 0.9987 - loss: 0.0729 - precision: 0.9708 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9833 - auc: 0.9987 - loss: 0.0725 - precision: 0.9712 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9833 - auc: 0.9987 - loss: 0.0720 - precision: 0.9717 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9833 - auc: 0.9986 - loss: 0.0717 - precision: 0.9721 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 78ms/step - accuracy: 0.9828 - auc: 0.9966 - loss: 0.0677 - precision: 0.9781 - recall: 0.9874 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.2105 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "20/20 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 111ms/step - accuracy: 0.9688 - auc: 1.0000 - loss: 0.0734 - precision: 0.9500 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 84ms/step - accuracy: 0.9766 - auc: 1.0000 - loss: 0.0598 - precision: 0.9615 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 75ms/step - accuracy: 0.9809 - auc: 1.0000 - loss: 0.0519 - precision: 0.9682 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 76ms/step - accuracy: 0.9818 - auc: 0.9988 - loss: 0.0548 - precision: 0.9693 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 76ms/step - accuracy: 0.9829 - auc: 0.9983 - loss: 0.0549 - precision: 0.9710 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 73ms/step - accuracy: 0.9840 - auc: 0.9981 - loss: 0.0541 - precision: 0.9726 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 72ms/step - accuracy: 0.9850 - auc: 0.9981 - loss: 0.0527 - precision: 0.9741 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 70ms/step - accuracy: 0.9854 - auc: 0.9980 - loss: 0.0528 - precision: 0.9743 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9859 - auc: 0.9979 - loss: 0.0524 - precision: 0.9749 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9864 - auc: 0.9979 - loss: 0.0517 - precision: 0.9755 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9866 - auc: 0.9978 - loss: 0.0514 - precision: 0.9762 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9868 - auc: 0.9978 - loss: 0.0508 - precision: 0.9769 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9871 - auc: 0.9978 - loss: 0.0504 - precision: 0.9776 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9874 - auc: 0.9978 - loss: 0.0498 - precision: 0.9782 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9875 - auc: 0.9978 - loss: 0.0495 - precision: 0.9786 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9877 - auc: 0.9978 - loss: 0.0491 - precision: 0.9790 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9879 - auc: 0.9979 - loss: 0.0486 - precision: 0.9794 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9881 - auc: 0.9979 - loss: 0.0481 - precision: 0.9798 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9882 - auc: 0.9979 - loss: 0.0476 - precision: 0.9800 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9882 - auc: 0.9979 - loss: 0.0474 - precision: 0.9800 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 81ms/step - accuracy: 0.9891 - auc: 0.9982 - loss: 0.0428 - precision: 0.9814 - recall: 0.9968 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.1434 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "20/20 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 100ms/step - accuracy: 0.9688 - auc: 1.0000 - loss: 0.0426 - precision: 0.9444 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 67ms/step - accuracy: 0.9766 - auc: 1.0000 - loss: 0.0384 - precision: 0.9591 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 67ms/step - accuracy: 0.9809 - auc: 1.0000 - loss: 0.0352 - precision: 0.9662 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 66ms/step - accuracy: 0.9837 - auc: 0.9999 - loss: 0.0350 - precision: 0.9711 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9857 - auc: 0.9999 - loss: 0.0349 - precision: 0.9746 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9872 - auc: 0.9999 - loss: 0.0341 - precision: 0.9772 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9884 - auc: 0.9999 - loss: 0.0330 - precision: 0.9793 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9894 - auc: 0.9999 - loss: 0.0321 - precision: 0.9810 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9902 - auc: 0.9999 - loss: 0.0313 - precision: 0.9824 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9908 - auc: 0.9999 - loss: 0.0305 - precision: 0.9835 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9914 - auc: 0.9999 - loss: 0.0298 - precision: 0.9845 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9917 - auc: 0.9997 - loss: 0.0302 - precision: 0.9849 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9918 - auc: 0.9995 - loss: 0.0306 - precision: 0.9850 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9917 - auc: 0.9993 - loss: 0.0316 - precision: 0.9848 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9917 - auc: 0.9991 - loss: 0.0323 - precision: 0.9847 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9916 - auc: 0.9989 - loss: 0.0329 - precision: 0.9845 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9916 - auc: 0.9988 - loss: 0.0333 - precision: 0.9844 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9916 - auc: 0.9987 - loss: 0.0335 - precision: 0.9843 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9916 - auc: 0.9986 - loss: 0.0337 - precision: 0.9842 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9916 - auc: 0.9985 - loss: 0.0339 - precision: 0.9843 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 79ms/step - accuracy: 0.9922 - auc: 0.9973 - loss: 0.0365 - precision: 0.9845 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.1292 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "20/20 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 103ms/step - accuracy: 0.9688 - auc: 1.0000 - loss: 0.0535 - precision: 0.9167 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 68ms/step - accuracy: 0.9766 - auc: 1.0000 - loss: 0.0419 - precision: 0.9391 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 67ms/step - accuracy: 0.9774 - auc: 0.9997 - loss: 0.0482 - precision: 0.9523 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 66ms/step - accuracy: 0.9772 - auc: 0.9996 - loss: 0.0501 - precision: 0.9603 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9768 - auc: 0.9995 - loss: 0.0514 - precision: 0.9633 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9763 - auc: 0.9994 - loss: 0.0519 - precision: 0.9644 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9759 - auc: 0.9993 - loss: 0.0524 - precision: 0.9648 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9759 - auc: 0.9993 - loss: 0.0520 - precision: 0.9657 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9763 - auc: 0.9993 - loss: 0.0512 - precision: 0.9665 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9768 - auc: 0.9993 - loss: 0.0502 - precision: 0.9675 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9774 - auc: 0.9993 - loss: 0.0490 - precision: 0.9685 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9779 - auc: 0.9994 - loss: 0.0480 - precision: 0.9694 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9785 - auc: 0.9994 - loss: 0.0469 - precision: 0.9703 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9791 - auc: 0.9994 - loss: 0.0458 - precision: 0.9712 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9797 - auc: 0.9994 - loss: 0.0449 - precision: 0.9721 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9801 - auc: 0.9994 - loss: 0.0441 - precision: 0.9727 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9805 - auc: 0.9995 - loss: 0.0433 - precision: 0.9732 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9808 - auc: 0.9995 - loss: 0.0426 - precision: 0.9736 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9811 - auc: 0.9995 - loss: 0.0420 - precision: 0.9739 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9814 - auc: 0.9995 - loss: 0.0413 - precision: 0.9743 - recall: 0.98 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 79ms/step - accuracy: 0.9875 - auc: 0.9998 - loss: 0.0287 - precision: 0.9813 - recall: 0.9937 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0846 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "20/20 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 100ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0106 - precision: 1.0000 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 67ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0089 - precision: 1.0000 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 66ms/step - accuracy: 0.9965 - auc: 1.0000 - loss: 0.0140 - precision: 1.0000 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 64ms/step - accuracy: 0.9954 - auc: 1.0000 - loss: 0.0155 - precision: 1.0000 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9951 - auc: 1.0000 - loss: 0.0158 - precision: 1.0000 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9951 - auc: 1.0000 - loss: 0.0158 - precision: 1.0000 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9951 - auc: 1.0000 - loss: 0.0157 - precision: 1.0000 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9952 - auc: 1.0000 - loss: 0.0159 - precision: 1.0000 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9954 - auc: 1.0000 - loss: 0.0160 - precision: 1.0000 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9949 - auc: 1.0000 - loss: 0.0165 - precision: 0.9988 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9946 - auc: 1.0000 - loss: 0.0169 - precision: 0.9979 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9942 - auc: 0.9998 - loss: 0.0181 - precision: 0.9968 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9939 - auc: 0.9997 - loss: 0.0191 - precision: 0.9960 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9937 - auc: 0.9996 - loss: 0.0198 - precision: 0.9953 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9936 - auc: 0.9995 - loss: 0.0203 - precision: 0.9948 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9935 - auc: 0.9994 - loss: 0.0207 - precision: 0.9944 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9933 - auc: 0.9994 - loss: 0.0211 - precision: 0.9940 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9932 - auc: 0.9994 - loss: 0.0215 - precision: 0.9938 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9931 - auc: 0.9993 - loss: 0.0217 - precision: 0.9936 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9931 - auc: 0.9993 - loss: 0.0220 - precision: 0.9934 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 80ms/step - accuracy: 0.9922 - auc: 0.9988 - loss: 0.0267 - precision: 0.9906 - recall: 0.9937 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0585 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "20/20 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 107ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0073 - precision: 1.0000 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 69ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0076 - precision: 1.0000 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 67ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0072 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 65ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0070 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0068 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0066 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0065 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0064 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9996 - auc: 1.0000 - loss: 0.0076 - precision: 0.9992 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9993 - auc: 0.9999 - loss: 0.0085 - precision: 0.9986 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 64ms/step - accuracy: 0.9991 - auc: 0.9999 - loss: 0.0091 - precision: 0.9982 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 64ms/step - accuracy: 0.9990 - auc: 0.9999 - loss: 0.0097 - precision: 0.9979 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9989 - auc: 0.9999 - loss: 0.0102 - precision: 0.9977 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9988 - auc: 0.9998 - loss: 0.0105 - precision: 0.9976 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9987 - auc: 0.9998 - loss: 0.0108 - precision: 0.9974 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9987 - auc: 0.9998 - loss: 0.0110 - precision: 0.9974 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9987 - auc: 0.9998 - loss: 0.0111 - precision: 0.9973 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9987 - auc: 0.9998 - loss: 0.0112 - precision: 0.9972 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9986 - auc: 0.9998 - loss: 0.0113 - precision: 0.9972 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9986 - auc: 0.9998 - loss: 0.0113 - precision: 0.9972 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 79ms/step - accuracy: 0.9984 - auc: 0.9998 - loss: 0.0121 - precision: 0.9969 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0327 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "20/20 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 99ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0053 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 69ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0047 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 67ms/step - accuracy: 0.9965 - auc: 1.0000 - loss: 0.0139 - precision: 0.9935 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 66ms/step - accuracy: 0.9954 - auc: 1.0000 - loss: 0.0167 - precision: 0.9914 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9951 - auc: 1.0000 - loss: 0.0175 - precision: 0.9908 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 65ms/step - accuracy: 0.9951 - auc: 1.0000 - loss: 0.0175 - precision: 0.9907 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9951 - auc: 1.0000 - loss: 0.0173 - precision: 0.9908 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9948 - auc: 1.0000 - loss: 0.0178 - precision: 0.9900 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 66ms/step - accuracy: 0.9946 - auc: 1.0000 - loss: 0.0180 - precision: 0.9896 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9945 - auc: 1.0000 - loss: 0.0180 - precision: 0.9894 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9945 - auc: 1.0000 - loss: 0.0180 - precision: 0.9894 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9945 - auc: 1.0000 - loss: 0.0179 - precision: 0.9894 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9945 - auc: 1.0000 - loss: 0.0178 - precision: 0.9895 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9946 - auc: 1.0000 - loss: 0.0176 - precision: 0.9896 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9947 - auc: 1.0000 - loss: 0.0174 - precision: 0.9897 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9947 - auc: 1.0000 - loss: 0.0175 - precision: 0.9896 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9947 - auc: 1.0000 - loss: 0.0175 - precision: 0.9895 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9945 - auc: 1.0000 - loss: 0.0178 - precision: 0.9895 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9943 - auc: 1.0000 - loss: 0.0180 - precision: 0.9896 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9942 - auc: 0.9999 - loss: 0.0182 - precision: 0.9896 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 80ms/step - accuracy: 0.9922 - auc: 0.9998 - loss: 0.0215 - precision: 0.9906 - recall: 0.9937 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0209 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "20/20 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 96ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0042 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 68ms/step - accuracy: 0.9922 - auc: 1.0000 - loss: 0.0143 - precision: 0.9844 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 68ms/step - accuracy: 0.9913 - auc: 1.0000 - loss: 0.0155 - precision: 0.9825 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 67ms/step - accuracy: 0.9915 - auc: 1.0000 - loss: 0.0153 - precision: 0.9828 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 76ms/step - accuracy: 0.9920 - auc: 1.0000 - loss: 0.0146 - precision: 0.9836 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 74ms/step - accuracy: 0.9924 - auc: 1.0000 - loss: 0.0139 - precision: 0.9845 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 72ms/step - accuracy: 0.9929 - auc: 1.0000 - loss: 0.0133 - precision: 0.9853 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 71ms/step - accuracy: 0.9933 - auc: 1.0000 - loss: 0.0127 - precision: 0.9861 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 70ms/step - accuracy: 0.9936 - auc: 1.0000 - loss: 0.0122 - precision: 0.9868 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 70ms/step - accuracy: 0.9940 - auc: 1.0000 - loss: 0.0118 - precision: 0.9874 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9943 - auc: 1.0000 - loss: 0.0115 - precision: 0.9880 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9945 - auc: 1.0000 - loss: 0.0113 - precision: 0.9886 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9948 - auc: 1.0000 - loss: 0.0110 - precision: 0.9891 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9950 - auc: 1.0000 - loss: 0.0108 - precision: 0.9895 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9952 - auc: 1.0000 - loss: 0.0105 - precision: 0.9900 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9954 - auc: 1.0000 - loss: 0.0103 - precision: 0.9903 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9955 - auc: 1.0000 - loss: 0.0101 - precision: 0.9907 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9957 - auc: 1.0000 - loss: 0.0099 - precision: 0.9910 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9958 - auc: 1.0000 - loss: 0.0097 - precision: 0.9913 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9959 - auc: 1.0000 - loss: 0.0095 - precision: 0.9916 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 81ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0062 - precision: 0.9969 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0085 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "20/20 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 108ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0039 - precision: 1.0000 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 69ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0032 - precision: 1.0000 - recall: 1.000 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 68ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0037 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 68ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0037 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 67ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0036 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0036 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0035 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0034 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0034 - precision: 1.0000 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.0047 - precision: 0.9994 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9995 - auc: 0.9999 - loss: 0.0056 - precision: 0.9989 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9993 - auc: 0.9999 - loss: 0.0063 - precision: 0.9986 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9992 - auc: 0.9999 - loss: 0.0068 - precision: 0.9983 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 69ms/step - accuracy: 0.9991 - auc: 0.9999 - loss: 0.0072 - precision: 0.9981 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9990 - auc: 0.9999 - loss: 0.0074 - precision: 0.9979 - recall: 1.00 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9988 - auc: 0.9999 - loss: 0.0078 - precision: 0.9978 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 68ms/step - accuracy: 0.9987 - auc: 0.9999 - loss: 0.0081 - precision: 0.9977 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9985 - auc: 0.9999 - loss: 0.0083 - precision: 0.9976 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0085 - precision: 0.9976 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 67ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0087 - precision: 0.9975 - recall: 0.99 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2s 80ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0114 - precision: 0.9968 - recall: 0.9968 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0052 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "‚úì Training completed!\n",
      "\n",
      "======================================================================\n",
      " MODEL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "üìä Generating predictions...\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001FDBF093F60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001FDBF093F60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Test Metrics:\n",
      "   Accuracy:  0.9950 (99.50%)\n",
      "   AUC-ROC:   1.0000\n",
      "   F1-Score:  0.9950\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Benign       1.00      0.99      0.99       100\n",
      "Steganography       0.99      1.00      1.00       100\n",
      "\n",
      "     accuracy                           0.99       200\n",
      "    macro avg       1.00      0.99      0.99       200\n",
      " weighted avg       1.00      0.99      0.99       200\n",
      "\n",
      "\n",
      "üîç Confusion Matrix:\n",
      "   True Negatives:      99\n",
      "   False Positives:      1\n",
      "   False Negatives:      0\n",
      "   True Positives:     100\n",
      "\n",
      "üìà Generating visualizations...\n",
      "‚úì Saved to: results\\bilstm_results.png\n",
      "\n",
      "======================================================================\n",
      " SAVING MODEL AND ARTIFACTS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Model saved: models\\bilstm_stego_20251016_115730.h5\n",
      "‚úì Scaler saved: models\\scaler_20251016_115730.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 23-98: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     36\u001b[39m classifier.plot_results(y_pred, y_pred_proba, cm)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m ‚úÖ BiLSTM Training Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 324\u001b[39m, in \u001b[36mBiLSTMClassifier.save_model\u001b[39m\u001b[34m(self, output_dir)\u001b[39m\n\u001b[32m    322\u001b[39m summary_path = os.path.join(output_dir, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmodel_summary_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(summary_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprint_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Model summary saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Save metadata\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\CNSProject\\env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 324\u001b[39m, in \u001b[36mBiLSTMClassifier.save_model.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    322\u001b[39m summary_path = os.path.join(output_dir, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmodel_summary_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(summary_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.summary(print_fn=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    325\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Model summary saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Save metadata\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1252.py:19\u001b[39m, in \u001b[36mIncrementalEncoder.encode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'charmap' codec can't encode characters in position 23-98: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    benign_pcap = \"output_86k.pcap\"\n",
    "    stego_pcap = \"steganography_dataset_20251016_020211.pcap\"\n",
    "    \n",
    "    # Initialize classifier\n",
    "    classifier = BiLSTMClassifier()\n",
    "    \n",
    "    # Load sequences from PCAP files\n",
    "    X, y, processor = classifier.load_sequences(\n",
    "        benign_pcap, stego_pcap,\n",
    "        max_flows_per_class=500,  # Reduced for memory\n",
    "        sequence_length=50\n",
    "    )\n",
    "    \n",
    "    # Preprocess\n",
    "    classifier.preprocess_sequences(X, y)\n",
    "    \n",
    "    # Build BiLSTM model\n",
    "    classifier.build_bilstm_model(\n",
    "        sequence_length=50,\n",
    "        n_features=13\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    classifier.train_model(\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred, y_pred_proba, cm = classifier.evaluate_model()\n",
    "    \n",
    "    # Visualize\n",
    "    classifier.plot_results(y_pred, y_pred_proba, cm)\n",
    "    \n",
    "    # Save\n",
    "    classifier.save_model()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" ‚úÖ BiLSTM Training Complete!\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9308baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
