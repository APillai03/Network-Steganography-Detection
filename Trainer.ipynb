{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "138c0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, roc_curve, auc, accuracy_score,precision_recall_curve, f1_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b49325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiLSTMSequenceProcessor:\n",
    "    \n",
    "    def __init__(self, sequence_length=50):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def create_sequences_from_pcap(self, pcap_file, label, max_flows=1000, packets_per_flow=50):\n",
    "        from scapy.all import PcapReader, IP, TCP, UDP\n",
    "        \n",
    "        print(f\"\\nProcessing {pcap_file} into sequences...\")\n",
    "        \n",
    "        flow_dict = {}\n",
    "        sequences = []\n",
    "        labels = []\n",
    "        \n",
    "        try:\n",
    "            with PcapReader(pcap_file) as pcap:\n",
    "                for pkt_idx, packet in enumerate(tqdm(pcap, desc=\"   Reading packets\")):\n",
    "                    if IP not in packet:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract flow identifier\n",
    "                    src_ip = packet[IP].src\n",
    "                    dst_ip = packet[IP].dst\n",
    "                    proto = packet[IP].proto\n",
    "                    \n",
    "                    if TCP in packet:\n",
    "                        src_port = packet[TCP].sport\n",
    "                        dst_port = packet[TCP].dport\n",
    "                    elif UDP in packet:\n",
    "                        src_port = packet[UDP].sport\n",
    "                        dst_port = packet[UDP].dport\n",
    "                    else:\n",
    "                        src_port = 0\n",
    "                        dst_port = 0\n",
    "                    \n",
    "                    # Create flow key (bidirectional)\n",
    "                    flow_key = tuple(sorted([\n",
    "                        (src_ip, src_port, proto),\n",
    "                        (dst_ip, dst_port, proto)\n",
    "                    ]))\n",
    "                    \n",
    "                    # Extract packet features for sequence\n",
    "                    features = self._extract_sequence_features(packet)\n",
    "                    \n",
    "                    if flow_key not in flow_dict:\n",
    "                        flow_dict[flow_key] = []\n",
    "                    \n",
    "                    flow_dict[flow_key].append(features)\n",
    "            \n",
    "            # Convert flows to sequences\n",
    "            print(f\"\\n   Converting {len(flow_dict)} flows to sequences...\")\n",
    "            for flow_key, packets in tqdm(flow_dict.items(), desc=\"   Creating sequences\"):\n",
    "                if len(packets) >= 3:  # Minimum 3 packets per flow\n",
    "                    # Pad or truncate to sequence_length\n",
    "                    if len(packets) < self.sequence_length:\n",
    "                        # Pad with zeros\n",
    "                        packets = packets + [[0]*13] * (self.sequence_length - len(packets))\n",
    "                    else:\n",
    "                        # Take first sequence_length packets\n",
    "                        packets = packets[:self.sequence_length]\n",
    "                    \n",
    "                    sequences.append(np.array(packets))\n",
    "                    labels.append(label)\n",
    "                    \n",
    "                    if len(sequences) >= max_flows:\n",
    "                        break\n",
    "            \n",
    "            print(f\"   Created {len(sequences)} sequences\")\n",
    "            return np.array(sequences), np.array(labels)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "            return np.array([]), np.array([])\n",
    "    \n",
    "    def _extract_sequence_features(self, packet):\n",
    "        from scapy.all import IP, TCP, UDP, ICMP, DNS\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        try:\n",
    "            if IP in packet:\n",
    "                # Packet length\n",
    "                features.append(len(packet) / 1500.0)  # Normalize\n",
    "                \n",
    "                # IP TTL\n",
    "                features.append(packet[IP].ttl / 255.0)\n",
    "                \n",
    "                # IP flags\n",
    "                features.append(int(packet[IP].flags) / 7.0)\n",
    "                \n",
    "                # IP ID (variation indicator)\n",
    "                features.append((packet[IP].id % 1000) / 1000.0)\n",
    "                \n",
    "                if TCP in packet:\n",
    "                    # TCP sport/dport ratio\n",
    "                    features.append(packet[TCP].sport / 65535.0)\n",
    "                    features.append(packet[TCP].dport / 65535.0)\n",
    "                    \n",
    "                    # TCP flags\n",
    "                    features.append(int(packet[TCP].flags) / 63.0)\n",
    "                    \n",
    "                    # TCP window size\n",
    "                    features.append(packet[TCP].window / 65535.0)\n",
    "                    \n",
    "                    # Payload length\n",
    "                    payload_len = len(packet[TCP].payload)\n",
    "                    features.append(payload_len / 1500.0)\n",
    "                    \n",
    "                    # Has options\n",
    "                    features.append(float(len(packet[TCP].options)))\n",
    "                    \n",
    "                    # Entropy indicator\n",
    "                    features.append(self._payload_entropy(bytes(packet[TCP].payload)))\n",
    "                    \n",
    "                    features.extend([0.0, 0.0])  # Padding for UDP\n",
    "                    \n",
    "                elif UDP in packet:\n",
    "                    # UDP sport/dport\n",
    "                    features.append(packet[UDP].sport / 65535.0)\n",
    "                    features.append(packet[UDP].dport / 65535.0)\n",
    "                    \n",
    "                    # Payload length\n",
    "                    payload_len = len(packet[UDP].payload)\n",
    "                    features.append(payload_len / 1500.0)\n",
    "                    \n",
    "                    features.extend([0.0, 0.0, 0.0, 0.0, 0.0])  # Padding for TCP\n",
    "                    \n",
    "                else:\n",
    "                    features.extend([0.0] * 10)\n",
    "                \n",
    "                # ICMP check\n",
    "                if ICMP in packet:\n",
    "                    features.append(1.0)\n",
    "                else:\n",
    "                    features.append(0.0)\n",
    "                \n",
    "                # DNS check\n",
    "                if DNS in packet:\n",
    "                    features.append(1.0)\n",
    "                else:\n",
    "                    features.append(0.0)\n",
    "            else:\n",
    "                features = [0.0] * 13\n",
    "        \n",
    "        except:\n",
    "            features = [0.0] * 13\n",
    "        \n",
    "        return features[:13]  # Ensure exactly 13 features\n",
    "    \n",
    "    def _payload_entropy(self, payload):\n",
    "        if not payload or len(payload) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        from scipy.stats import entropy\n",
    "        byte_counts = np.bincount(list(payload), minlength=256)\n",
    "        probabilities = byte_counts[byte_counts > 0] / len(payload)\n",
    "        ent = entropy(probabilities, base=2) / 8.0  # Normalize to 0-1\n",
    "        return min(ent, 1.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbc9425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.scaler_x = StandardScaler()\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        \n",
    "    def load_sequences(self, benign_pcap, stego_pcap, max_flows_per_class=1000, \n",
    "                       sequence_length=50):\n",
    "        print(\" LOADING PACKET SEQUENCES\")\n",
    "        \n",
    "        processor = BiLSTMSequenceProcessor(sequence_length=sequence_length)\n",
    "        \n",
    "        # Process benign traffic\n",
    "        X_benign, y_benign = processor.create_sequences_from_pcap(\n",
    "            benign_pcap, label=0, max_flows=max_flows_per_class\n",
    "        )\n",
    "        \n",
    "        # Process steganographic traffic\n",
    "        X_stego, y_stego = processor.create_sequences_from_pcap(\n",
    "            stego_pcap, label=1, max_flows=max_flows_per_class\n",
    "        )\n",
    "        \n",
    "        # Combine datasets\n",
    "        X = np.concatenate([X_benign, X_stego], axis=0)\n",
    "        y = np.concatenate([y_benign, y_stego], axis=0)\n",
    "        \n",
    "        print(f\"\\n✓ Total sequences: {len(X)}\")\n",
    "        print(f\"  - Benign: {len(X_benign)}\")\n",
    "        print(f\"  - Steganography: {len(X_stego)}\")\n",
    "        print(f\"  - Sequence length: {sequence_length}\")\n",
    "        print(f\"  - Features per packet: 13\")\n",
    "        \n",
    "        return X, y, processor\n",
    "    \n",
    "    def preprocess_sequences(self, X, y, test_size=0.2, random_state=42):\n",
    "        print(\" PREPROCESSING SEQUENCES\")\n",
    "        \n",
    "        print(f\"\\nSplitting sequences...\")\n",
    "        \n",
    "        # Train-test split\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"   Train: {len(self.X_train)} sequences\")\n",
    "        print(f\"   Test:  {len(self.X_test)} sequences\")\n",
    "        \n",
    "        # Normalize sequences (per timestep)\n",
    "        print(f\"\\nNormalizing sequences...\")\n",
    "        seq_len, n_features = self.X_train[0].shape\n",
    "        \n",
    "        # Reshape for scaling\n",
    "        X_train_reshaped = self.X_train.reshape(-1, n_features)\n",
    "        X_test_reshaped = self.X_test.reshape(-1, n_features)\n",
    "        \n",
    "        X_train_scaled = self.scaler_x.fit_transform(X_train_reshaped)\n",
    "        X_test_scaled = self.scaler_x.transform(X_test_reshaped)\n",
    "        \n",
    "        self.X_train = X_train_scaled.reshape(self.X_train.shape)\n",
    "        self.X_test = X_test_scaled.reshape(self.X_test.shape)\n",
    "        \n",
    "        print(f\"   Sequences normalized\")\n",
    "        print(f\"   Shape: {self.X_train.shape}\")\n",
    "    \n",
    "    def build_bilstm_model(self, sequence_length=50, n_features=13):\n",
    "        \"\"\"Build BiLSTM model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" BUILDING BiLSTM MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\nArchitecture:\")\n",
    "        print(f\"   Input shape: ({sequence_length}, {n_features})\")\n",
    "        \n",
    "        self.model = models.Sequential([\n",
    "            # BiLSTM layers\n",
    "            layers.Input(shape=(sequence_length, n_features)),\n",
    "            \n",
    "            # First BiLSTM block\n",
    "            layers.Bidirectional(\n",
    "                layers.LSTM(128, return_sequences=True, dropout=0.2),\n",
    "                name='bilstm_1'\n",
    "            ),\n",
    "            layers.Dropout(0.3, name='dropout_1'),\n",
    "            \n",
    "            # Second BiLSTM block\n",
    "            layers.Bidirectional(\n",
    "                layers.LSTM(64, return_sequences=True, dropout=0.2),\n",
    "                name='bilstm_2'\n",
    "            ),\n",
    "            layers.Dropout(0.2, name='dropout_2'),\n",
    "            \n",
    "            # Third BiLSTM block\n",
    "            layers.Bidirectional(\n",
    "                layers.LSTM(32, return_sequences=False, dropout=0.2),\n",
    "                name='bilstm_3'\n",
    "            ),\n",
    "            layers.Dropout(0.2, name='dropout_3'),\n",
    "            \n",
    "            # Dense layers\n",
    "            layers.Dense(64, activation='relu', name='dense_1'),\n",
    "            layers.BatchNormalization(name='bn_1'),\n",
    "            layers.Dropout(0.2, name='dropout_4'),\n",
    "            \n",
    "            layers.Dense(32, activation='relu', name='dense_2'),\n",
    "            layers.Dropout(0.1, name='dropout_5'),\n",
    "            \n",
    "            # Output\n",
    "            layers.Dense(1, activation='sigmoid', name='output')\n",
    "        ])\n",
    "        \n",
    "        print(\"   Layers:\")\n",
    "        print(\"   ├─ BiLSTM: 128 (return_seq=True, dropout=0.2) → Dropout(0.3)\")\n",
    "        print(\"   ├─ BiLSTM: 64 (return_seq=True, dropout=0.2) → Dropout(0.2)\")\n",
    "        print(\"   ├─ BiLSTM: 32 (return_seq=False, dropout=0.2) → Dropout(0.2)\")\n",
    "        print(\"   ├─ Dense: 64 → ReLU → BatchNorm → Dropout(0.2)\")\n",
    "        print(\"   ├─ Dense: 32 → ReLU → Dropout(0.1)\")\n",
    "        print(\"   └─ Output: 1 → Sigmoid\")\n",
    "        \n",
    "        # Compile\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                keras.metrics.AUC(name='auc'),\n",
    "                keras.metrics.Precision(name='precision'),\n",
    "                keras.metrics.Recall(name='recall')\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"\\nModel compiled\")\n",
    "        print(\"   Optimizer: Adam (lr=0.001)\")\n",
    "        print(\"   Loss: Binary Crossentropy\")\n",
    "    \n",
    "    def train_model(self, epochs=100, batch_size=32, validation_split=0.2):\n",
    "        print(\" TRAINING BiLSTM MODEL\")\n",
    "        \n",
    "        print(f\"\\nParameters:\")\n",
    "        print(f\"   Epochs: {epochs}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Validation split: {validation_split*100:.0f}%\")\n",
    "        \n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=20,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=10,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nStarting training...\\n\")\n",
    "        \n",
    "        self.history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\n✓ Training completed!\")\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        print(\" MODEL EVALUATION\")\n",
    "        \n",
    "        print(\"\\nGenerating predictions...\")\n",
    "        y_pred_proba = self.model.predict(self.X_test, verbose=0)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        auc_score = roc_auc_score(self.y_test, y_pred_proba)\n",
    "        f1 = f1_score(self.y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\nTest Metrics:\")\n",
    "        print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"   AUC-ROC:   {auc_score:.4f}\")\n",
    "        print(f\"   F1-Score:  {f1:.4f}\")\n",
    "        \n",
    "        print(f\"\\nClassification Report:\")\n",
    "        print(classification_report(\n",
    "            self.y_test, y_pred,\n",
    "            target_names=['Benign', 'Steganography']\n",
    "        ))\n",
    "        \n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(f\"   True Negatives:  {cm[0,0]:>6,}\")\n",
    "        print(f\"   False Positives: {cm[0,1]:>6,}\")\n",
    "        print(f\"   False Negatives: {cm[1,0]:>6,}\")\n",
    "        print(f\"   True Positives:  {cm[1,1]:>6,}\")\n",
    "        \n",
    "        return y_pred, y_pred_proba, cm\n",
    "    \n",
    "    def plot_results(self, y_pred, y_pred_proba, cm, output_dir='results'):\n",
    "        \"\"\"Plot training and evaluation results\"\"\"\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Training history\n",
    "        ax1 = plt.subplot(2, 3, 1)\n",
    "        ax1.plot(self.history.history['accuracy'], label='Train', linewidth=2)\n",
    "        ax1.plot(self.history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "        ax1.set_title('Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2 = plt.subplot(2, 3, 2)\n",
    "        ax2.plot(self.history.history['loss'], label='Train', linewidth=2)\n",
    "        ax2.plot(self.history.history['val_loss'], label='Validation', linewidth=2)\n",
    "        ax2.set_title('Loss', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        ax3.plot(self.history.history['auc'], label='Train', linewidth=2)\n",
    "        ax3.plot(self.history.history['val_auc'], label='Validation', linewidth=2)\n",
    "        ax3.set_title('AUC-ROC', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('AUC')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax4,\n",
    "                    xticklabels=['Benign', 'Stego'],\n",
    "                    yticklabels=['Benign', 'Stego'])\n",
    "        ax4.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # ROC curve\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax5.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.4f})', linewidth=2)\n",
    "        ax5.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "        ax5.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "        ax5.set_xlabel('False Positive Rate')\n",
    "        ax5.set_ylabel('True Positive Rate')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # PR curve\n",
    "        ax6 = plt.subplot(2, 3, 6)\n",
    "        precision, recall, _ = precision_recall_curve(self.y_test, y_pred_proba)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        ax6.plot(recall, precision, label=f'PR (AUC = {pr_auc:.4f})', linewidth=2)\n",
    "        ax6.set_title('Precision-Recall', fontsize=12, fontweight='bold')\n",
    "        ax6.set_xlabel('Recall')\n",
    "        ax6.set_ylabel('Precision')\n",
    "        ax6.legend()\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = os.path.join(output_dir, 'bilstm_results.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\" Saved to: {plot_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Add this method to the BiLSTMClassifier class in train_bilstm.py\n",
    "\n",
    "    def save_model(self, output_dir='models'):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        print(\" SAVING MODEL AND ARTIFACTS\")\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(output_dir, f'bilstm_stego_{timestamp}.h5')\n",
    "        self.model.save(model_path)\n",
    "        print(f\"\\n Model saved: {model_path}\")\n",
    "        \n",
    "        # Save scaler (IMPORTANT!)\n",
    "        scaler_path = os.path.join(output_dir, f'scaler_{timestamp}.pkl')\n",
    "        import pickle\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(self.scaler_x, f)\n",
    "        print(f\"✓ Scaler saved: {scaler_path}\")\n",
    "        \n",
    "        # Save model summary\n",
    "        summary_path = os.path.join(output_dir, f'model_summary_{timestamp}.txt')\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            self.model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        print(f\" Model summary saved: {summary_path}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'timestamp': timestamp,\n",
    "            'sequence_length': 50,\n",
    "            'n_features': 13,\n",
    "            'model_path': model_path,\n",
    "            'scaler_path': scaler_path,\n",
    "            'training_date': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(output_dir, f'metadata_{timestamp}.pkl')\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        print(f\"✓ Metadata saved: {metadata_path}\")\n",
    "        \n",
    "        print(\" FILES SAVED IN ./models/\")\n",
    "        print(f\"\\n Files created:\")\n",
    "        print(f\"   1. {os.path.basename(model_path)}\")\n",
    "        print(f\"   2. {os.path.basename(scaler_path)}\")\n",
    "        print(f\"   3. {os.path.basename(summary_path)}\")\n",
    "        print(f\"   4. {os.path.basename(metadata_path)}\")\n",
    "        print(f\"\\n Use these for inference:\")\n",
    "        print(f\"   model_path = '{model_path}'\")\n",
    "        print(f\"   scaler_path = '{scaler_path}'\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b31564d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LOADING PACKET SEQUENCES\n",
      "\n",
      "Processing Dataset/Benign_Dump.pcap into sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Reading packets: 86000it [00:20, 4119.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Converting 38444 flows to sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Creating sequences:  49%|████▉     | 18752/38444 [00:00<00:00, 506573.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 500 sequences\n",
      "\n",
      "Processing Dataset/steganography_dataset_20251016_233034.pcap into sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Reading packets: 92067it [00:35, 2564.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Converting 81985 flows to sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Creating sequences:  14%|█▎        | 11187/81985 [00:00<00:00, 360190.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created 500 sequences\n",
      "\n",
      "✓ Total sequences: 1000\n",
      "  - Benign: 500\n",
      "  - Steganography: 500\n",
      "  - Sequence length: 50\n",
      "  - Features per packet: 13\n",
      " PREPROCESSING SEQUENCES\n",
      "\n",
      "Splitting sequences...\n",
      "   Train: 800 sequences\n",
      "   Test:  200 sequences\n",
      "\n",
      "Normalizing sequences...\n",
      "   Sequences normalized\n",
      "   Shape: (800, 50, 13)\n",
      "\n",
      "======================================================================\n",
      " BUILDING BiLSTM MODEL\n",
      "======================================================================\n",
      "\n",
      "Architecture:\n",
      "   Input shape: (50, 13)\n",
      "   Layers:\n",
      "   ├─ BiLSTM: 128 (return_seq=True, dropout=0.2) → Dropout(0.3)\n",
      "   ├─ BiLSTM: 64 (return_seq=True, dropout=0.2) → Dropout(0.2)\n",
      "   ├─ BiLSTM: 32 (return_seq=False, dropout=0.2) → Dropout(0.2)\n",
      "   ├─ Dense: 64 → ReLU → BatchNorm → Dropout(0.2)\n",
      "   ├─ Dense: 32 → ReLU → Dropout(0.1)\n",
      "   └─ Output: 1 → Sigmoid\n",
      "\n",
      "Model compiled\n",
      "   Optimizer: Adam (lr=0.001)\n",
      "   Loss: Binary Crossentropy\n",
      " TRAINING BiLSTM MODEL\n",
      "\n",
      "Parameters:\n",
      "   Epochs: 10\n",
      "   Batch size: 32\n",
      "   Validation split: 20%\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/10\n",
      "20/20 ━━━━━━━━━━━━━━━━━━━━ 2:03 6s/step - accuracy: 0.5000 - auc: 0.5263 - loss: 0.7209 - precision: 0.3333 - recall: 0.23 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.5642 - auc: 0.6253 - loss: 0.6710 - precision: 0.4716 - recall: 0.37 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.6239 - auc: 0.6965 - loss: 0.6140 - precision: 0.5595 - recall: 0.50 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.6658 - auc: 0.7451 - loss: 0.5697 - precision: 0.6117 - recall: 0.58 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.6971 - auc: 0.7797 - loss: 0.5341 - precision: 0.6477 - recall: 0.63 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.7221 - auc: 0.8060 - loss: 0.5039 - precision: 0.6758 - recall: 0.68 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.7425 - auc: 0.8268 - loss: 0.4776 - precision: 0.6979 - recall: 0.71 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.7514 - auc: 0.8354 - loss: 0.4661 - precision: 0.7072 - recall: 0.72 ━━━━━━━━━━━━━━━━━━━━ 0s 43ms/step - accuracy: 0.7592 - auc: 0.8430 - loss: 0.4557 - precision: 0.7156 - recall: 0.73 ━━━━━━━━━━━━━━━━━━━━ 0s 43ms/step - accuracy: 0.7733 - auc: 0.8564 - loss: 0.4368 - precision: 0.7313 - recall: 0.75 ━━━━━━━━━━━━━━━━━━━━ 0s 43ms/step - accuracy: 0.7856 - auc: 0.8677 - loss: 0.4197 - precision: 0.7455 - recall: 0.77 ━━━━━━━━━━━━━━━━━━━━ 0s 44ms/step - accuracy: 0.7912 - auc: 0.8727 - loss: 0.4116 - precision: 0.7519 - recall: 0.78 ━━━━━━━━━━━━━━━━━━━━ 8s 100ms/step - accuracy: 0.8984 - auc: 0.9687 - loss: 0.2581 - precision: 0.8750 - recall: 0.9274 - val_accuracy: 0.9812 - val_auc: 1.0000 - val_loss: 0.4096 - val_precision: 0.9651 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "20/20 ━━━━━━━━━━━━━━━━━━━━ 1s 61ms/step - accuracy: 0.9375 - auc: 0.9913 - loss: 0.1805 - precision: 1.0000 - recall: 0.90 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9427 - auc: 0.9831 - loss: 0.1749 - precision: 0.9621 - recall: 0.93 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9475 - auc: 0.9850 - loss: 0.1604 - precision: 0.9524 - recall: 0.94 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9522 - auc: 0.9867 - loss: 0.1484 - precision: 0.9510 - recall: 0.95 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9563 - auc: 0.9881 - loss: 0.1385 - precision: 0.9519 - recall: 0.96 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9596 - auc: 0.9892 - loss: 0.1304 - precision: 0.9534 - recall: 0.96 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9620 - auc: 0.9901 - loss: 0.1247 - precision: 0.9543 - recall: 0.97 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9631 - auc: 0.9904 - loss: 0.1221 - precision: 0.9549 - recall: 0.97 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9642 - auc: 0.9908 - loss: 0.1196 - precision: 0.9557 - recall: 0.97 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9656 - auc: 0.9912 - loss: 0.1159 - precision: 0.9566 - recall: 0.97 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9669 - auc: 0.9917 - loss: 0.1123 - precision: 0.9576 - recall: 0.97 ━━━━━━━━━━━━━━━━━━━━ 1s 49ms/step - accuracy: 0.9797 - auc: 0.9960 - loss: 0.0778 - precision: 0.9691 - recall: 0.9905 - val_accuracy: 0.9875 - val_auc: 1.0000 - val_loss: 0.2413 - val_precision: 0.9765 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "20/20 ━━━━━━━━━━━━━━━━━━━━ 1s 60ms/step - accuracy: 0.9688 - auc: 1.0000 - loss: 0.0708 - precision: 0.9000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9653 - auc: 0.9982 - loss: 0.0768 - precision: 0.9320 - recall: 0.98 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9679 - auc: 0.9981 - loss: 0.0732 - precision: 0.9416 - recall: 0.98 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9716 - auc: 0.9982 - loss: 0.0676 - precision: 0.9497 - recall: 0.98 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9746 - auc: 0.9984 - loss: 0.0624 - precision: 0.9558 - recall: 0.98 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9762 - auc: 0.9985 - loss: 0.0595 - precision: 0.9588 - recall: 0.98 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9776 - auc: 0.9985 - loss: 0.0574 - precision: 0.9611 - recall: 0.98 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9789 - auc: 0.9985 - loss: 0.0553 - precision: 0.9633 - recall: 0.98 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9794 - auc: 0.9985 - loss: 0.0543 - precision: 0.9644 - recall: 0.98 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9804 - auc: 0.9986 - loss: 0.0527 - precision: 0.9663 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9812 - auc: 0.9986 - loss: 0.0514 - precision: 0.9681 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 1s 49ms/step - accuracy: 0.9875 - auc: 0.9987 - loss: 0.0410 - precision: 0.9843 - recall: 0.9905 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.1733 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "20/20 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0259 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0292 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0270 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0251 - precision: 0.9986 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0238 - precision: 0.9972 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9978 - auc: 1.0000 - loss: 0.0238 - precision: 0.9954 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9974 - auc: 1.0000 - loss: 0.0236 - precision: 0.9944 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.0232 - precision: 0.9939 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9971 - auc: 1.0000 - loss: 0.0229 - precision: 0.9937 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9970 - auc: 0.9999 - loss: 0.0227 - precision: 0.9936 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9970 - auc: 0.9999 - loss: 0.0224 - precision: 0.9936 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 1s 49ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0192 - precision: 0.9937 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.1167 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "20/20 ━━━━━━━━━━━━━━━━━━━━ 1s 61ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0036 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9965 - auc: 1.0000 - loss: 0.0136 - precision: 1.0000 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9939 - auc: 0.9998 - loss: 0.0212 - precision: 0.9973 - recall: 0.98 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9928 - auc: 0.9997 - loss: 0.0234 - precision: 0.9950 - recall: 0.98 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9928 - auc: 0.9997 - loss: 0.0233 - precision: 0.9944 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9930 - auc: 0.9997 - loss: 0.0230 - precision: 0.9943 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9931 - auc: 0.9997 - loss: 0.0231 - precision: 0.9939 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9932 - auc: 0.9997 - loss: 0.0231 - precision: 0.9936 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9933 - auc: 0.9997 - loss: 0.0231 - precision: 0.9935 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9934 - auc: 0.9998 - loss: 0.0230 - precision: 0.9934 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9936 - auc: 0.9998 - loss: 0.0227 - precision: 0.9934 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 1s 49ms/step - accuracy: 0.9953 - auc: 0.9998 - loss: 0.0198 - precision: 0.9937 - recall: 0.9968 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0670 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "20/20 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0058 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9913 - auc: 0.9986 - loss: 0.0368 - precision: 0.9837 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9920 - auc: 0.9985 - loss: 0.0357 - precision: 0.9851 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9915 - auc: 0.9987 - loss: 0.0336 - precision: 0.9867 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9914 - auc: 0.9988 - loss: 0.0316 - precision: 0.9873 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9913 - auc: 0.9989 - loss: 0.0297 - precision: 0.9875 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9914 - auc: 0.9990 - loss: 0.0288 - precision: 0.9877 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 43ms/step - accuracy: 0.9915 - auc: 0.9990 - loss: 0.0280 - precision: 0.9879 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9915 - auc: 0.9991 - loss: 0.0273 - precision: 0.9878 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9916 - auc: 0.9992 - loss: 0.0267 - precision: 0.9879 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9918 - auc: 0.9992 - loss: 0.0260 - precision: 0.9881 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 1s 49ms/step - accuracy: 0.9937 - auc: 0.9996 - loss: 0.0191 - precision: 0.9906 - recall: 0.9968 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0775 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "20/20 ━━━━━━━━━━━━━━━━━━━━ 1s 61ms/step - accuracy: 0.9688 - auc: 1.0000 - loss: 0.0413 - precision: 0.9444 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9774 - auc: 0.9999 - loss: 0.0298 - precision: 0.9639 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9808 - auc: 0.9998 - loss: 0.0272 - precision: 0.9729 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9835 - auc: 0.9998 - loss: 0.0253 - precision: 0.9779 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9856 - auc: 0.9998 - loss: 0.0236 - precision: 0.9812 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9871 - auc: 0.9998 - loss: 0.0220 - precision: 0.9835 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9877 - auc: 0.9998 - loss: 0.0213 - precision: 0.9845 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 43ms/step - accuracy: 0.9881 - auc: 0.9998 - loss: 0.0210 - precision: 0.9849 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9889 - auc: 0.9998 - loss: 0.0203 - precision: 0.9858 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9895 - auc: 0.9998 - loss: 0.0196 - precision: 0.9866 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9901 - auc: 0.9999 - loss: 0.0189 - precision: 0.9873 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 1s 49ms/step - accuracy: 0.9953 - auc: 0.9999 - loss: 0.0125 - precision: 0.9937 - recall: 0.9968 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0387 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "20/20 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0059 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0050 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.0094 - precision: 1.0000 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9966 - auc: 1.0000 - loss: 0.0105 - precision: 1.0000 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9965 - auc: 1.0000 - loss: 0.0106 - precision: 1.0000 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9961 - auc: 1.0000 - loss: 0.0115 - precision: 1.0000 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9959 - auc: 1.0000 - loss: 0.0119 - precision: 1.0000 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9959 - auc: 1.0000 - loss: 0.0120 - precision: 1.0000 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9958 - auc: 1.0000 - loss: 0.0121 - precision: 0.9998 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9957 - auc: 1.0000 - loss: 0.0121 - precision: 0.9994 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 1s 49ms/step - accuracy: 0.9922 - auc: 0.9998 - loss: 0.0202 - precision: 0.9906 - recall: 0.9937 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0178 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "20/20 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0027 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0030 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0032 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0033 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0035 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0035 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9998 - auc: 1.0000 - loss: 0.0039 - precision: 1.0000 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0055 - precision: 0.9994 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0061 - precision: 0.9992 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0066 - precision: 0.9990 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0073 - precision: 0.9988 - recall: 0.99 ━━━━━━━━━━━━━━━━━━━━ 1s 49ms/step - accuracy: 0.9953 - auc: 0.9999 - loss: 0.0123 - precision: 0.9968 - recall: 0.9937 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0135 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "20/20 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0012 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0023 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0023 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0024 - precision: 1.0000 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9996 - auc: 1.0000 - loss: 0.0033 - precision: 0.9993 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0047 - precision: 0.9979 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.0069 - precision: 0.9960 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - accuracy: 0.9970 - auc: 1.0000 - loss: 0.0086 - precision: 0.9942 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9967 - auc: 1.0000 - loss: 0.0093 - precision: 0.9936 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9963 - auc: 1.0000 - loss: 0.0103 - precision: 0.9928 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step - accuracy: 0.9960 - auc: 0.9999 - loss: 0.0110 - precision: 0.9922 - recall: 1.00 ━━━━━━━━━━━━━━━━━━━━ 1s 49ms/step - accuracy: 0.9937 - auc: 0.9999 - loss: 0.0171 - precision: 0.9875 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0388 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\n",
      "✓ Training completed!\n",
      " MODEL EVALUATION\n",
      "\n",
      "Generating predictions...\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002622456D760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002622456D760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Metrics:\n",
      "   Accuracy:  0.9900 (99.00%)\n",
      "   AUC-ROC:   1.0000\n",
      "   F1-Score:  0.9901\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Benign       1.00      0.98      0.99       100\n",
      "Steganography       0.98      1.00      0.99       100\n",
      "\n",
      "     accuracy                           0.99       200\n",
      "    macro avg       0.99      0.99      0.99       200\n",
      " weighted avg       0.99      0.99      0.99       200\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "   True Negatives:      98\n",
      "   False Positives:      2\n",
      "   False Negatives:      0\n",
      "   True Positives:     100\n",
      "\n",
      "Generating visualizations...\n",
      " Saved to: results\\bilstm_results.png\n",
      " SAVING MODEL AND ARTIFACTS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model saved: models\\bilstm_stego_20251017_000558.h5\n",
      "✓ Scaler saved: models\\scaler_20251017_000558.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model summary saved: models\\model_summary_20251017_000558.txt\n",
      "✓ Metadata saved: models\\metadata_20251017_000558.pkl\n",
      " FILES SAVED IN ./models/\n",
      "\n",
      " Files created:\n",
      "   1. bilstm_stego_20251017_000558.h5\n",
      "   2. scaler_20251017_000558.pkl\n",
      "   3. model_summary_20251017_000558.txt\n",
      "   4. metadata_20251017_000558.pkl\n",
      "\n",
      " Use these for inference:\n",
      "   model_path = 'models\\bilstm_stego_20251017_000558.h5'\n",
      "   scaler_path = 'models\\scaler_20251017_000558.pkl'\n",
      "BiLSTM Training Complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    benign_pcap = \"Dataset/Benign_Dump.pcap\"\n",
    "    stego_pcap = \"Dataset/steganography_dataset_20251016_233034.pcap\"\n",
    "    \n",
    "    classifier = BiLSTMClassifier()\n",
    "    \n",
    "    X, y, processor = classifier.load_sequences(\n",
    "        benign_pcap, stego_pcap,\n",
    "        max_flows_per_class=500,  # Reduced for memory\n",
    "        sequence_length=50\n",
    "    )\n",
    "    \n",
    "    classifier.preprocess_sequences(X, y)\n",
    "    \n",
    "    classifier.build_bilstm_model(\n",
    "        sequence_length=50,\n",
    "        n_features=13\n",
    "    )\n",
    "    \n",
    "    classifier.train_model(\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    y_pred, y_pred_proba, cm = classifier.evaluate_model()\n",
    "    \n",
    "    classifier.plot_results(y_pred, y_pred_proba, cm)\n",
    "    \n",
    "    # Save\n",
    "    classifier.save_model()\n",
    "    \n",
    "    print(\"BiLSTM Training Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5edb2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
