{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60150255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scapy.all import PcapReader, IP, TCP, UDP, ICMP, DNS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0292585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BiLSTMInference:\n",
    "    \"\"\"Load trained BiLSTM model and perform inference on real traffic\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, scaler_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained model and scaler\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to .h5 model file\n",
    "            scaler_path: Path to .pkl scaler file\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" LOADING TRAINED MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load model\n",
    "        print(f\"\\nðŸ“¦ Loading model from: {model_path}\")\n",
    "        self.model = keras.models.load_model(model_path)\n",
    "        print(f\"âœ“ Model loaded successfully\")\n",
    "        \n",
    "        # Load scaler\n",
    "        print(f\"ðŸ“¦ Loading scaler from: {scaler_path}\")\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            self.scaler_x = pickle.load(f)\n",
    "        print(f\"âœ“ Scaler loaded successfully\")\n",
    "        \n",
    "        # Model info\n",
    "        print(f\"\\nðŸ“Š Model Information:\")\n",
    "        print(f\"   Total parameters: {self.model.count_params():,}\")\n",
    "        \n",
    "        self.sequence_length = 50\n",
    "        self.n_features = 13\n",
    "    \n",
    "    def extract_sequence_features(self, packet):\n",
    "        \"\"\"Extract 13 features from a single packet\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        try:\n",
    "            if IP in packet:\n",
    "                # Packet length (normalized to 1500 max)\n",
    "                features.append(min(len(packet) / 1500.0, 1.0))\n",
    "                \n",
    "                # IP TTL (normalized to 255 max)\n",
    "                features.append(packet[IP].ttl / 255.0)\n",
    "                \n",
    "                # IP flags\n",
    "                features.append(int(packet[IP].flags) / 7.0)\n",
    "                \n",
    "                # IP ID variation\n",
    "                features.append((packet[IP].id % 1000) / 1000.0)\n",
    "                \n",
    "                if TCP in packet:\n",
    "                    # TCP ports\n",
    "                    features.append(packet[TCP].sport / 65535.0)\n",
    "                    features.append(packet[TCP].dport / 65535.0)\n",
    "                    \n",
    "                    # TCP flags\n",
    "                    features.append(int(packet[TCP].flags) / 63.0)\n",
    "                    \n",
    "                    # TCP window size\n",
    "                    features.append(packet[TCP].window / 65535.0)\n",
    "                    \n",
    "                    # Payload length\n",
    "                    payload_len = len(packet[TCP].payload)\n",
    "                    features.append(min(payload_len / 1500.0, 1.0))\n",
    "                    \n",
    "                    # TCP options count\n",
    "                    features.append(min(float(len(packet[TCP].options)) / 10.0, 1.0))\n",
    "                    \n",
    "                    # Payload entropy\n",
    "                    features.append(self._payload_entropy(bytes(packet[TCP].payload)))\n",
    "                    \n",
    "                    features.extend([0.0, 0.0])\n",
    "                    \n",
    "                elif UDP in packet:\n",
    "                    features.append(packet[UDP].sport / 65535.0)\n",
    "                    features.append(packet[UDP].dport / 65535.0)\n",
    "                    \n",
    "                    payload_len = len(packet[UDP].payload)\n",
    "                    features.append(min(payload_len / 1500.0, 1.0))\n",
    "                    \n",
    "                    features.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "                    \n",
    "                else:\n",
    "                    features.extend([0.0] * 10)\n",
    "                \n",
    "                # ICMP\n",
    "                features.append(1.0 if ICMP in packet else 0.0)\n",
    "                \n",
    "                # DNS\n",
    "                features.append(1.0 if DNS in packet else 0.0)\n",
    "            else:\n",
    "                features = [0.0] * 13\n",
    "        \n",
    "        except:\n",
    "            features = [0.0] * 13\n",
    "        \n",
    "        return np.array(features[:13])\n",
    "    \n",
    "    def _payload_entropy(self, payload):\n",
    "        \"\"\"Calculate normalized Shannon entropy\"\"\"\n",
    "        if not payload or len(payload) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        from scipy.stats import entropy\n",
    "        byte_counts = np.bincount(list(payload), minlength=256)\n",
    "        probabilities = byte_counts[byte_counts > 0] / len(payload)\n",
    "        ent = entropy(probabilities, base=2) / 8.0\n",
    "        return min(ent, 1.0)\n",
    "    \n",
    "    def extract_flows_from_pcap(self, pcap_file):\n",
    "        \"\"\"Extract flows from PCAP file\"\"\"\n",
    "        print(f\"\\nðŸ“‚ Reading PCAP: {pcap_file}\")\n",
    "        \n",
    "        flow_dict = defaultdict(list)\n",
    "        \n",
    "        try:\n",
    "            with PcapReader(pcap_file) as pcap:\n",
    "                for packet in tqdm(pcap, desc=\"   Extracting flows\"):\n",
    "                    if IP not in packet:\n",
    "                        continue\n",
    "                    \n",
    "                    # Flow identifier\n",
    "                    src_ip = packet[IP].src\n",
    "                    dst_ip = packet[IP].dst\n",
    "                    proto = packet[IP].proto\n",
    "                    \n",
    "                    if TCP in packet:\n",
    "                        src_port = packet[TCP].sport\n",
    "                        dst_port = packet[TCP].dport\n",
    "                    elif UDP in packet:\n",
    "                        src_port = packet[UDP].sport\n",
    "                        dst_port = packet[UDP].dport\n",
    "                    else:\n",
    "                        src_port = 0\n",
    "                        dst_port = 0\n",
    "                    \n",
    "                    flow_key = tuple(sorted([\n",
    "                        (src_ip, src_port, proto),\n",
    "                        (dst_ip, dst_port, proto)\n",
    "                    ]))\n",
    "                    \n",
    "                    # Extract features\n",
    "                    features = self.extract_sequence_features(packet)\n",
    "                    flow_dict[flow_key].append(features)\n",
    "            \n",
    "            print(f\"   âœ“ Extracted {len(flow_dict)} flows\")\n",
    "            return flow_dict\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error reading PCAP: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def create_sequences_from_flows(self, flow_dict):\n",
    "        \"\"\"Convert flows to sequences\"\"\"\n",
    "        print(f\"\\nðŸ”„ Creating sequences from flows...\")\n",
    "        \n",
    "        sequences = []\n",
    "        flow_labels = []\n",
    "        \n",
    "        for flow_key, packets in tqdm(flow_dict.items(), desc=\"   Converting\"):\n",
    "            if len(packets) >= 3:  # Minimum 3 packets\n",
    "                # Pad or truncate\n",
    "                if len(packets) < self.sequence_length:\n",
    "                    packets = packets + [np.zeros(13)] * (self.sequence_length - len(packets))\n",
    "                else:\n",
    "                    packets = packets[:self.sequence_length]\n",
    "                \n",
    "                seq = np.array(packets)\n",
    "                sequences.append(seq)\n",
    "                flow_labels.append(flow_key)\n",
    "        \n",
    "        sequences = np.array(sequences)\n",
    "        print(f\"   âœ“ Created {len(sequences)} sequences\")\n",
    "        \n",
    "        # Normalize\n",
    "        print(f\"   Normalizing sequences...\")\n",
    "        seq_len, n_feat = sequences[0].shape\n",
    "        X_reshaped = sequences.reshape(-1, n_feat)\n",
    "        X_scaled = self.scaler_x.transform(X_reshaped)\n",
    "        sequences = X_scaled.reshape(sequences.shape)\n",
    "        \n",
    "        return sequences, flow_labels\n",
    "    \n",
    "    def predict_flows(self, sequences, flow_labels, threshold=0.5):\n",
    "        \"\"\"Predict steganography on sequences\"\"\"\n",
    "        print(f\"\\nðŸ”® Running inference on {len(sequences)} sequences...\")\n",
    "        \n",
    "        predictions = self.model.predict(sequences, verbose=0)\n",
    "        pred_labels = (predictions > threshold).astype(int).flatten()\n",
    "        pred_proba = predictions.flatten()\n",
    "        \n",
    "        results = []\n",
    "        for flow_key, pred, prob in zip(flow_labels, pred_labels, pred_proba):\n",
    "            result = {\n",
    "                'flow': f\"{flow_key[0][0]}:{flow_key[0][1]} â†” {flow_key[1][0]}:{flow_key[1][1]}\",\n",
    "                'protocol': flow_key[0][2],\n",
    "                'prediction': 'STEGANOGRAPHY' if pred == 1 else 'BENIGN',\n",
    "                'confidence': prob if pred == 1 else (1 - prob),\n",
    "                'raw_probability': prob\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_results(self, results):\n",
    "        \"\"\"Analyze and display results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" INFERENCE RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        stego_count = sum(1 for r in results if r['prediction'] == 'STEGANOGRAPHY')\n",
    "        benign_count = len(results) - stego_count\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Summary:\")\n",
    "        print(f\"   Total flows analyzed: {len(results)}\")\n",
    "        print(f\"   Steganography detected: {stego_count} ({stego_count/len(results)*100:.1f}%)\")\n",
    "        print(f\"   Benign flows: {benign_count} ({benign_count/len(results)*100:.1f}%)\")\n",
    "        \n",
    "        # Sort by confidence\n",
    "        results_sorted = sorted(results, key=lambda x: x['confidence'], reverse=True)\n",
    "        \n",
    "        print(f\"\\nðŸ”´ HIGH CONFIDENCE STEGANOGRAPHY FLOWS:\")\n",
    "        stego_high = [r for r in results_sorted if r['prediction'] == 'STEGANOGRAPHY' and r['confidence'] > 0.9]\n",
    "        if stego_high:\n",
    "            for i, r in enumerate(stego_high[:10], 1):\n",
    "                print(f\"   {i}. {r['flow']}\")\n",
    "                print(f\"      Confidence: {r['confidence']:.4f} | Proto: {r['protocol']}\")\n",
    "        else:\n",
    "            print(f\"   None detected\")\n",
    "        \n",
    "        print(f\"\\nðŸŸ¢ HIGH CONFIDENCE BENIGN FLOWS:\")\n",
    "        benign_high = [r for r in results_sorted if r['prediction'] == 'BENIGN' and r['confidence'] > 0.9]\n",
    "        if benign_high:\n",
    "            for i, r in enumerate(benign_high[:10], 1):\n",
    "                print(f\"   {i}. {r['flow']}\")\n",
    "                print(f\"      Confidence: {r['confidence']:.4f} | Proto: {r['protocol']}\")\n",
    "        else:\n",
    "            print(f\"   None detected\")\n",
    "        \n",
    "        print(f\"\\nðŸŸ¡ UNCERTAIN FLOWS (0.4 < confidence < 0.6):\")\n",
    "        uncertain = [r for r in results_sorted if 0.4 < r['confidence'] < 0.6]\n",
    "        if uncertain:\n",
    "            for i, r in enumerate(uncertain[:10], 1):\n",
    "                print(f\"   {i}. {r['flow']}\")\n",
    "                print(f\"      Confidence: {r['confidence']:.4f} | Prediction: {r['prediction']}\")\n",
    "        else:\n",
    "            print(f\"   None detected\")\n",
    "        \n",
    "        return results_sorted\n",
    "    \n",
    "    def export_results(self, results, output_csv='inference_results.csv'):\n",
    "        \"\"\"Export results to CSV\"\"\"\n",
    "        print(f\"\\nðŸ’¾ Exporting results...\")\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"   âœ“ Saved to: {output_csv}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def visualize_predictions(self, results, output_dir='results'):\n",
    "        \"\"\"Visualize prediction distribution\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Creating visualizations...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Prediction distribution\n",
    "        preds = [r['prediction'] for r in results]\n",
    "        pred_counts = pd.Series(preds).value_counts()\n",
    "        \n",
    "        axes[0, 0].bar(pred_counts.index, pred_counts.values, color=['#2ecc71', '#e74c3c'])\n",
    "        axes[0, 0].set_title('Prediction Distribution', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "        for i, v in enumerate(pred_counts.values):\n",
    "            axes[0, 0].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "        \n",
    "        # Confidence distribution\n",
    "        confidences = [r['confidence'] for r in results]\n",
    "        axes[0, 1].hist(confidences, bins=30, color='#3498db', edgecolor='black')\n",
    "        axes[0, 1].set_title('Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Confidence')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].axvline(np.mean(confidences), color='red', linestyle='--', label=f'Mean: {np.mean(confidences):.2f}')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Confidence by prediction\n",
    "        stego_conf = [r['confidence'] for r in results if r['prediction'] == 'STEGANOGRAPHY']\n",
    "        benign_conf = [r['confidence'] for r in results if r['prediction'] == 'BENIGN']\n",
    "        \n",
    "        axes[1, 0].boxplot([stego_conf, benign_conf], labels=['Stego', 'Benign'])\n",
    "        axes[1, 0].set_title('Confidence by Prediction', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('Confidence')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ROC-like curve\n",
    "        raw_probs = sorted([r['raw_probability'] for r in results])\n",
    "        axes[1, 1].plot(raw_probs, color='#9b59b6', linewidth=2)\n",
    "        axes[1, 1].axhline(0.5, color='red', linestyle='--', label='Decision Boundary')\n",
    "        axes[1, 1].set_title('Raw Probability Distribution', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Sample Index (sorted)')\n",
    "        axes[1, 1].set_ylabel('Probability of Steganography')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, 'inference_visualization.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"   âœ“ Saved to: {plot_path}\")\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30679676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " LOADING TRAINED MODEL\n",
      "======================================================================\n",
      "\n",
      "ðŸ“¦ Loading model from: models/bilstm_stego_20251017_210507.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded successfully\n",
      "ðŸ“¦ Loading scaler from: models/scaler_20251017_210507.pkl\n",
      "âœ“ Scaler loaded successfully\n",
      "\n",
      "ðŸ“Š Model Information:\n",
      "   Total parameters: 357,505\n",
      "\n",
      "ðŸ“‚ Reading PCAP: archive/PCAP/RITA pcap files/1.pcap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting flows: 148895it [01:11, 2069.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Extracted 957 flows\n",
      "\n",
      "ðŸ”„ Creating sequences from flows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Converting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 957/957 [00:00<00:00, 75764.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Created 538 sequences\n",
      "   Normalizing sequences...\n",
      "\n",
      "ðŸ”® Running inference on 538 sequences...\n",
      "\n",
      "======================================================================\n",
      " INFERENCE RESULTS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "   Total flows analyzed: 538\n",
      "   Steganography detected: 121 (22.5%)\n",
      "   Benign flows: 417 (77.5%)\n",
      "\n",
      "ðŸ”´ HIGH CONFIDENCE STEGANOGRAPHY FLOWS:\n",
      "   1. 192.168.43.188:68 â†” 192.168.43.49:67\n",
      "      Confidence: 0.9947 | Proto: 17\n",
      "   2. 142.251.143.129:443 â†” 192.168.43.188:58914\n",
      "      Confidence: 0.9939 | Proto: 17\n",
      "   3. 142.251.143.106:443 â†” 192.168.43.188:64330\n",
      "      Confidence: 0.9938 | Proto: 17\n",
      "   4. 142.250.180.163:443 â†” 192.168.43.188:61369\n",
      "      Confidence: 0.9936 | Proto: 17\n",
      "   5. 142.251.209.42:443 â†” 192.168.43.188:53115\n",
      "      Confidence: 0.9935 | Proto: 17\n",
      "   6. 142.250.180.142:443 â†” 192.168.43.188:52058\n",
      "      Confidence: 0.9934 | Proto: 17\n",
      "   7. 142.251.209.42:443 â†” 192.168.43.188:63383\n",
      "      Confidence: 0.9934 | Proto: 17\n",
      "   8. 142.250.180.142:443 â†” 192.168.43.188:56596\n",
      "      Confidence: 0.9934 | Proto: 17\n",
      "   9. 142.251.143.206:443 â†” 192.168.43.188:51714\n",
      "      Confidence: 0.9932 | Proto: 17\n",
      "   10. 142.251.143.195:443 â†” 192.168.43.188:61923\n",
      "      Confidence: 0.9932 | Proto: 17\n",
      "\n",
      "ðŸŸ¢ HIGH CONFIDENCE BENIGN FLOWS:\n",
      "   1. 192.168.43.188:15475 â†” 52.33.236.211:443\n",
      "      Confidence: 0.9923 | Proto: 6\n",
      "   2. 192.168.43.188:10135 â†” 54.201.75.72:443\n",
      "      Confidence: 0.9918 | Proto: 6\n",
      "   3. 157.240.203.60:443 â†” 192.168.43.188:15493\n",
      "      Confidence: 0.9910 | Proto: 6\n",
      "   4. 192.168.43.188:10016 â†” 34.208.81.80:443\n",
      "      Confidence: 0.9908 | Proto: 6\n",
      "   5. 157.240.203.14:443 â†” 192.168.43.188:10134\n",
      "      Confidence: 0.9903 | Proto: 6\n",
      "   6. 157.240.203.14:443 â†” 192.168.43.188:10126\n",
      "      Confidence: 0.9900 | Proto: 6\n",
      "   7. 192.168.43.188:15479 â†” 34.208.81.80:443\n",
      "      Confidence: 0.9897 | Proto: 6\n",
      "   8. 192.168.43.188:10094 â†” 67.27.139.126:80\n",
      "      Confidence: 0.9891 | Proto: 6\n",
      "   9. 157.240.203.14:443 â†” 192.168.43.188:10002\n",
      "      Confidence: 0.9891 | Proto: 6\n",
      "   10. 172.65.247.109:443 â†” 192.168.43.188:9940\n",
      "      Confidence: 0.9890 | Proto: 6\n",
      "\n",
      "ðŸŸ¡ UNCERTAIN FLOWS (0.4 < confidence < 0.6):\n",
      "   1. 192.168.43.188:56488 â†” 216.58.209.33:443\n",
      "      Confidence: 0.5847 | Prediction: STEGANOGRAPHY\n",
      "   2. 142.251.143.165:443 â†” 192.168.43.188:10053\n",
      "      Confidence: 0.5839 | Prediction: STEGANOGRAPHY\n",
      "   3. 142.251.143.129:443 â†” 192.168.43.188:10184\n",
      "      Confidence: 0.5684 | Prediction: STEGANOGRAPHY\n",
      "   4. 142.251.143.106:443 â†” 192.168.43.188:61758\n",
      "      Confidence: 0.5595 | Prediction: BENIGN\n",
      "   5. 192.168.43.188:64556 â†” 216.58.209.54:443\n",
      "      Confidence: 0.5519 | Prediction: STEGANOGRAPHY\n",
      "   6. 104.208.16.0:443 â†” 192.168.43.188:9953\n",
      "      Confidence: 0.5504 | Prediction: BENIGN\n",
      "   7. 142.251.209.33:443 â†” 192.168.43.188:10176\n",
      "      Confidence: 0.5474 | Prediction: BENIGN\n",
      "   8. 157.240.203.2:443 â†” 192.168.43.188:65069\n",
      "      Confidence: 0.5416 | Prediction: STEGANOGRAPHY\n",
      "   9. 192.168.43.188:9936 â†” 20.50.201.200:443\n",
      "      Confidence: 0.5253 | Prediction: BENIGN\n",
      "   10. 142.251.143.138:443 â†” 192.168.43.188:10064\n",
      "      Confidence: 0.5213 | Prediction: BENIGN\n",
      "\n",
      "ðŸ’¾ Exporting results...\n",
      "   âœ“ Saved to: inference_results.csv\n",
      "\n",
      "ðŸ“ˆ Creating visualizations...\n",
      "   âœ“ Saved to: results\\inference_visualization.png\n",
      "\n",
      "======================================================================\n",
      " âœ… Inference Complete!\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main inference pipeline\"\"\"\n",
    "    \n",
    "    # Model paths (update these with your actual model files)\n",
    "    model_path = \"models/bilstm_stego_20251017_210507.h5\"  # Update with your model\n",
    "    scaler_path = \"models/scaler_20251017_210507.pkl\"      # Update with your scaler\n",
    "    \n",
    "    # Test PCAP\n",
    "    test_pcap = \"stego_dataset\\steganography_dataset_20251017_210123.pcap\"  # Your test traffic\n",
    "    \n",
    "    # Check if model exists\n",
    "    if not os.path.exists(model_path) or not os.path.exists(scaler_path):\n",
    "        print(\"âŒ Model files not found!\")\n",
    "        print(f\"   Looking for: {model_path}\")\n",
    "        print(f\"   Looking for: {scaler_path}\")\n",
    "        print(\"\\nPlease train a model first using train_bilstm.py\")\n",
    "        return\n",
    "    \n",
    "    # Initialize inference\n",
    "    inference = BiLSTMInference(model_path, scaler_path)\n",
    "    \n",
    "    # Extract flows\n",
    "    flow_dict = inference.extract_flows_from_pcap(test_pcap)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences, flow_labels = inference.create_sequences_from_flows(flow_dict)\n",
    "    \n",
    "    # Run inference\n",
    "    results = inference.predict_flows(sequences, flow_labels, threshold=0.5)\n",
    "    \n",
    "    # Analyze results\n",
    "    results_sorted = inference.analyze_results(results)\n",
    "    \n",
    "    # Export\n",
    "    df = inference.export_results(results_sorted)\n",
    "    \n",
    "    # Visualize\n",
    "    inference.visualize_predictions(results_sorted)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" âœ… Inference Complete!\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85cbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
