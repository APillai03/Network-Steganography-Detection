{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " EXTRACTING FEATURES FROM PCAPS\n",
      "======================================================================\n",
      "\n",
      "Processing Dataset/Benign_Dump.pcap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Reading packets: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Reading packets: 86000it [00:21, 4083.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Extracted 38444 flows\n",
      "   Flow size stats: min=1, max=15853, avg=2.2, median=1\n",
      "   Flows with â‰¥3 packets: 611/38444 (1.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Creating features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 1402.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Created 500 valid sequences from 500 flows\n",
      "\n",
      "Processing Dataset/steganography_dataset_20251016_233034.pcap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Reading packets: 92067it [00:35, 2585.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Extracted 81985 flows\n",
      "   Flow size stats: min=1, max=7, avg=1.1, median=1\n",
      "   Flows with â‰¥3 packets: 1062/81985 (1.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Creating features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 1393.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Created 500 valid sequences from 500 flows\n",
      "\n",
      "âœ“ Feature Extraction Complete:\n",
      "   Benign: 500 flows\n",
      "   Stego:  500 flows\n",
      "\n",
      "âœ“ Combined Dataset:\n",
      "   Sequences shape: (1000, 50, 13)\n",
      "   Statistical shape: (1000, 65)\n",
      "   Labels shape: (1000,)\n",
      "   Total samples: 1000\n",
      "\n",
      "ðŸ”€ Splitting data...\n",
      "âœ“ Data Split:\n",
      "   Train: 640 samples\n",
      "   Val:   160 samples\n",
      "   Test:  200 samples\n",
      "\n",
      "======================================================================\n",
      " BUILDING BiLSTM MODEL\n",
      "======================================================================\n",
      "âœ“ BiLSTM model built\n",
      "\n",
      "======================================================================\n",
      " BUILDING ML MODELS\n",
      "======================================================================\n",
      "âœ“ Random Forest model initialized\n",
      "âœ“ Gradient Boosting model initialized\n",
      "âœ“ Logistic Regression model initialized\n",
      "\n",
      "======================================================================\n",
      " TRAINING ENSEMBLE\n",
      "======================================================================\n",
      "\n",
      "Normalizing sequence features...\n",
      "Normalizing statistical features...\n",
      "\n",
      "[1/4] Training BiLSTM...\n",
      "Epoch 1/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1:58 6s/step - accuracy: 0.4375 - auc: 0.4039 - loss: 0.843 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 75ms/step - accuracy: 0.4688 - auc: 0.4436 - loss: 0.835 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 70ms/step - accuracy: 0.5139 - auc: 0.5042 - loss: 0.799 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 65ms/step - accuracy: 0.5514 - auc: 0.5573 - loss: 0.763 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 62ms/step - accuracy: 0.5861 - auc: 0.6037 - loss: 0.728 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 60ms/step - accuracy: 0.6152 - auc: 0.6415 - loss: 0.697 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 59ms/step - accuracy: 0.6376 - auc: 0.6719 - loss: 0.671 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 56ms/step - accuracy: 0.6743 - auc: 0.7202 - loss: 0.626 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 0.6900 - auc: 0.7399 - loss: 0.606 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 0.7043 - auc: 0.7573 - loss: 0.588 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 0.7170 - auc: 0.7721 - loss: 0.571 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 0.7392 - auc: 0.7973 - loss: 0.541 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 0.7488 - auc: 0.8079 - loss: 0.527 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 0.7577 - auc: 0.8176 - loss: 0.514 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 0.7657 - auc: 0.8262 - loss: 0.503 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 0.7733 - auc: 0.8341 - loss: 0.491 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 0.7803 - auc: 0.8413 - loss: 0.481 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 0.7867 - auc: 0.8479 - loss: 0.471 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 104ms/step - accuracy: 0.9094 - auc: 0.9731 - loss: 0.2783 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.3947 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 73ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.056 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9922 - auc: 1.0000 - loss: 0.066 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9913 - auc: 1.0000 - loss: 0.067 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9915 - auc: 1.0000 - loss: 0.066 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9920 - auc: 1.0000 - loss: 0.065 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9924 - auc: 1.0000 - loss: 0.064 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9923 - auc: 1.0000 - loss: 0.063 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9922 - auc: 1.0000 - loss: 0.062 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9923 - auc: 1.0000 - loss: 0.061 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9925 - auc: 1.0000 - loss: 0.060 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9926 - auc: 1.0000 - loss: 0.059 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9928 - auc: 1.0000 - loss: 0.058 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9930 - auc: 1.0000 - loss: 0.057 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9932 - auc: 1.0000 - loss: 0.056 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9934 - auc: 1.0000 - loss: 0.055 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9935 - auc: 1.0000 - loss: 0.054 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 0.9937 - auc: 1.0000 - loss: 0.053 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 0.9939 - auc: 1.0000 - loss: 0.052 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9941 - auc: 1.0000 - loss: 0.051 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 60ms/step - accuracy: 0.9969 - auc: 1.0000 - loss: 0.0357 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.2242 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 71ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.033 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.028 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.025 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9980 - auc: 1.0000 - loss: 0.024 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 50ms/step - accuracy: 0.9968 - auc: 1.0000 - loss: 0.022 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 0.9965 - auc: 1.0000 - loss: 0.021 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 50ms/step - accuracy: 0.9965 - auc: 1.0000 - loss: 0.021 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 50ms/step - accuracy: 0.9966 - auc: 1.0000 - loss: 0.020 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9966 - auc: 1.0000 - loss: 0.019 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9967 - auc: 1.0000 - loss: 0.019 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9968 - auc: 1.0000 - loss: 0.019 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9968 - auc: 1.0000 - loss: 0.018 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9969 - auc: 1.0000 - loss: 0.018 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9970 - auc: 1.0000 - loss: 0.017 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 0.9971 - auc: 1.0000 - loss: 0.017 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.017 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 59ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0121 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.1497 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 70ms/step - accuracy: 0.9688 - auc: 1.0000 - loss: 0.129 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 0.9766 - auc: 1.0000 - loss: 0.099 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 61ms/step - accuracy: 0.9809 - auc: 1.0000 - loss: 0.082 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 59ms/step - accuracy: 0.9837 - auc: 1.0000 - loss: 0.071 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 58ms/step - accuracy: 0.9857 - auc: 0.9999 - loss: 0.064 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 57ms/step - accuracy: 0.9872 - auc: 0.9999 - loss: 0.058 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 56ms/step - accuracy: 0.9884 - auc: 0.9999 - loss: 0.053 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 56ms/step - accuracy: 0.9894 - auc: 0.9999 - loss: 0.049 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 0.9902 - auc: 0.9999 - loss: 0.046 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 0.9908 - auc: 0.9999 - loss: 0.044 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 0.9912 - auc: 0.9998 - loss: 0.042 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 0.9918 - auc: 0.9998 - loss: 0.039 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 0.9923 - auc: 0.9998 - loss: 0.036 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 0.9925 - auc: 0.9998 - loss: 0.035 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 0.9927 - auc: 0.9998 - loss: 0.034 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 0.9929 - auc: 0.9998 - loss: 0.033 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 0.9931 - auc: 0.9998 - loss: 0.032 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 0.9933 - auc: 0.9998 - loss: 0.031 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 63ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0168 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.1102 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 67ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.005 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 0.9999 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 60ms/step - accuracy: 0.9984 - auc: 0.9993 - loss: 0.0149 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0651 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 69ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.010 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.008 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 61ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 58ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 56ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 59ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0053 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0424 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 73ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.014 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.011 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.009 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.008 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.008 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.008 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 57ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.008 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 50ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.006 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 55ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0049 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0256 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 84ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0041 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0148 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 64ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.008 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.005 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 42ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 42ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 42ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 0.9996 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.005 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.005 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 52ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0125 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0088 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 73ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9965 - auc: 0.9996 - loss: 0.021 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9951 - auc: 0.9995 - loss: 0.028 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9951 - auc: 0.9995 - loss: 0.028 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9954 - auc: 0.9995 - loss: 0.026 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9955 - auc: 0.9995 - loss: 0.025 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9957 - auc: 0.9995 - loss: 0.024 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9960 - auc: 0.9996 - loss: 0.023 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9962 - auc: 0.9996 - loss: 0.022 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9964 - auc: 0.9996 - loss: 0.020 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9966 - auc: 0.9996 - loss: 0.019 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 52ms/step - accuracy: 0.9984 - auc: 0.9998 - loss: 0.0104 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 68ms/step - accuracy: 0.9688 - auc: 1.0000 - loss: 0.106 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 0.9809 - auc: 0.9997 - loss: 0.066 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9818 - auc: 0.9996 - loss: 0.061 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 0.9829 - auc: 0.9996 - loss: 0.056 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9850 - auc: 0.9996 - loss: 0.048 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9867 - auc: 0.9996 - loss: 0.043 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9880 - auc: 0.9996 - loss: 0.039 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9891 - auc: 0.9997 - loss: 0.035 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9900 - auc: 0.9997 - loss: 0.033 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9905 - auc: 0.9997 - loss: 0.031 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9910 - auc: 0.9997 - loss: 0.029 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9912 - auc: 0.9997 - loss: 0.029 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 53ms/step - accuracy: 0.9953 - auc: 0.9999 - loss: 0.0149 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 68ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.020 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 42ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.013 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.011 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 50ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.010 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.008 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.008 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.009 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.009 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.009 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 0.9982 - auc: 1.0000 - loss: 0.009 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9980 - auc: 1.0000 - loss: 0.009 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.009 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 54ms/step - accuracy: 0.9969 - auc: 1.0000 - loss: 0.0099 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 64ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 42ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 42ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0034 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 65ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 7.5159e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 57ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0049    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 57ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.005 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.005 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.005 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9998 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 54ms/step - accuracy: 0.9969 - auc: 1.0000 - loss: 0.0065 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 6.6859e-04 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 69ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.3660e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 7.5096e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 9.0044e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 9.3439e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 9.8331e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0011    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0015 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 4.3398e-04 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 64ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.6676e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0014    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9999 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 52ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0040 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 2.4190e-04 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 65ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.0366e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0010    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9986 - auc: 1.0000 - loss: 0.005 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9977 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9973 - auc: 1.0000 - loss: 0.008 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9969 - auc: 1.0000 - loss: 0.010 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9966 - auc: 1.0000 - loss: 0.011 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9966 - auc: 1.0000 - loss: 0.011 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9966 - auc: 1.0000 - loss: 0.011 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9965 - auc: 1.0000 - loss: 0.011 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9966 - auc: 1.0000 - loss: 0.011 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 54ms/step - accuracy: 0.9969 - auc: 1.0000 - loss: 0.0109 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 2.1550e-04 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 67ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 0.9988 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 51ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0032 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 1.4541e-04 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 75ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 4.6516e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 55ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0011    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.005 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 53ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0083 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0011 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 66ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.060 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.037 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.027 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.022 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.020 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.019 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.017 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.016 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.015 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.014 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.013 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.012 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0049 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 5.7138e-05 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 64ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 7.1075e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.5351e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.2027e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 7.7416e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.0047e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.1091e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.0835e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.0375e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.0011    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 55ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0025 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 5.2057e-05 - learning_rate: 0.0010\n",
      "Epoch 22/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 76ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 1.6020e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0014    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 55ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0041 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 5.2660e-05 - learning_rate: 0.0010\n",
      "Epoch 23/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 67ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 2.3841e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.2683e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0015    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 53ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 50ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 50ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 58ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 3.6887e-05 - learning_rate: 0.0010\n",
      "Epoch 24/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 66ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 9.1856e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 7.8158e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 7.0710e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.6452e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.3774e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.0963e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.8998e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.8145e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.7353e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.5827e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 4.1341e-04 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 2.8549e-05 - learning_rate: 0.0010\n",
      "Epoch 25/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 67ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 4.0794e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.7103e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.4587e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.4680e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.3193e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.2857e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.2212e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.1823e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.1373e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.5779e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 4.1786e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 4.5597e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 4.6886e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 4.8810e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 56ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.4921e-04 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 2.3555e-05 - learning_rate: 0.0010\n",
      "Epoch 26/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 65ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 1.4764e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 1.2079e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 1.8827e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 2.0343e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 2.1384e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 0.9996 - auc: 0.9996 - loss: 0.0027    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 0.9991 - auc: 0.9991 - loss: 0.005 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 0.9989 - auc: 0.9989 - loss: 0.007 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9987 - auc: 0.9987 - loss: 0.008 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9987 - auc: 0.9986 - loss: 0.009 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 0.9986 - auc: 0.9986 - loss: 0.009 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 55ms/step - accuracy: 0.9984 - auc: 0.9984 - loss: 0.0107 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 6.6588e-05 - learning_rate: 0.0010\n",
      "Epoch 27/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 66ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.9361e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.7865e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.6740e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.2490e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.3255e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.6531e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.7470e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.8082e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.8412e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.8282e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.8245e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.7944e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 55ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.5152e-04 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 8.5842e-05 - learning_rate: 0.0010\n",
      "Epoch 28/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 80ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 1.6259e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.0903e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.5744e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.9149e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.9781e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.8883e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.6635e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.5645e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.4508e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.4999e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.4460e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.3986e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.3515e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.2983e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.5214e-04\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 58ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 5.4601e-05 - learning_rate: 0.0010\n",
      "Epoch 29/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 72ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 4.4672e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 58ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.4638e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 57ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.3834e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 56ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.1329e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 56ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.6072e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.7553e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.8930e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 5.8877e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.0449e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.1327e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 52ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.2333e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.3491e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 50ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.5632e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 50ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.8126e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 50ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.8908e-0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 58ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.3783e-04 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 3.6061e-05 - learning_rate: 5.0000e-04\n",
      "Epoch 30/30\n",
      "20/20 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 67ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.004 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 51ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.003 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.002 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 49ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 48ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 47ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 46ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.001 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 54ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.9108e-04 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 2.6160e-05 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "âœ“ BiLSTM training complete\n",
      "\n",
      "[2/4] Training Random Forest...\n",
      "âœ“ Random Forest validation accuracy: 1.0000\n",
      "\n",
      "[3/4] Training Gradient Boosting...\n",
      "âœ“ Gradient Boosting validation accuracy: 1.0000\n",
      "\n",
      "[4/4] Training Logistic Regression...\n",
      "âœ“ Logistic Regression validation accuracy: 1.0000\n",
      "\n",
      "======================================================================\n",
      " ENSEMBLE EVALUATION\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ¯ Ensemble Performance (weighted_average):\n",
      "   Accuracy:  1.0000 (100.00%)\n",
      "   F1-Score:  1.0000\n",
      "   AUC-ROC:   1.0000\n",
      "\n",
      "ðŸ“Š Individual Model Performance:\n",
      "   BILSTM     Acc: 1.0000  F1: 1.0000\n",
      "   RF         Acc: 1.0000  F1: 1.0000\n",
      "   GB         Acc: 1.0000  F1: 1.0000\n",
      "   LR         Acc: 1.0000  F1: 1.0000\n",
      "\n",
      "ðŸ“‹ Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Benign       1.00      1.00      1.00       100\n",
      "Steganography       1.00      1.00      1.00       100\n",
      "\n",
      "     accuracy                           1.00       200\n",
      "    macro avg       1.00      1.00      1.00       200\n",
      " weighted avg       1.00      1.00      1.00       200\n",
      "\n",
      "\n",
      "ðŸ”¢ Confusion Matrix:\n",
      "   True Negatives:     100\n",
      "   False Positives:      0\n",
      "   False Negatives:      0\n",
      "   True Positives:     100\n",
      "\n",
      "ðŸ“ˆ Generating visualizations...\n",
      "âœ“ Saved: results\\ensemble_results.png\n",
      "\n",
      "======================================================================\n",
      " SAVING ENSEMBLE MODELS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ BiLSTM saved: models\\ensemble_bilstm_20251107_004524.h5\n",
      "âœ“ ML models saved: models\\ensemble_models_20251107_004524.pkl\n",
      "\n",
      "âœ“ All models saved in models/\n",
      "\n",
      "======================================================================\n",
      " âœ… ENSEMBLE TRAINING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ Check ./results/ for visualizations\n",
      "ðŸ“ Check ./models/ for saved models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, f1_score, precision_recall_curve)\n",
    "from scapy.all import PcapReader, IP, TCP, UDP, ICMP, DNS\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class EnsembleFeatureExtractor:\n",
    "    def __init__(self, sequence_length=50):\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def extract_packet_features(self, packet):\n",
    "        features = []\n",
    "        \n",
    "        try:\n",
    "            if IP in packet:\n",
    "                features.append(min(len(packet) / 1500.0, 1.0))\n",
    "                features.append(packet[IP].ttl / 255.0)\n",
    "                features.append(int(packet[IP].flags) / 7.0)\n",
    "                features.append((packet[IP].id % 1000) / 1000.0)\n",
    "                \n",
    "                if TCP in packet:\n",
    "                    features.append(packet[TCP].sport / 65535.0)\n",
    "                    features.append(packet[TCP].dport / 65535.0)\n",
    "                    features.append(int(packet[TCP].flags) / 63.0)\n",
    "                    features.append(packet[TCP].window / 65535.0)\n",
    "                    payload_len = len(packet[TCP].payload)\n",
    "                    features.append(min(payload_len / 1500.0, 1.0))\n",
    "                    features.append(min(float(len(packet[TCP].options)) / 10.0, 1.0))\n",
    "                    features.append(self._payload_entropy(bytes(packet[TCP].payload)))\n",
    "                    features.extend([0.0, 0.0])\n",
    "                    \n",
    "                elif UDP in packet:\n",
    "                    features.append(packet[UDP].sport / 65535.0)\n",
    "                    features.append(packet[UDP].dport / 65535.0)\n",
    "                    payload_len = len(packet[UDP].payload)\n",
    "                    features.append(min(payload_len / 1500.0, 1.0))\n",
    "                    features.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "                    \n",
    "                else:\n",
    "                    features.extend([0.0] * 10)\n",
    "                \n",
    "                features.append(1.0 if ICMP in packet else 0.0)\n",
    "                features.append(1.0 if DNS in packet else 0.0)\n",
    "            else:\n",
    "                features = [0.0] * 13\n",
    "        except Exception as e:\n",
    "            # Debug: Print what failed\n",
    "            # print(f\"   Feature extraction error: {e}\")\n",
    "            features = [0.0] * 13\n",
    "        \n",
    "        # Ensure exactly 13 features\n",
    "        if len(features) < 13:\n",
    "            features.extend([0.0] * (13 - len(features)))\n",
    "        \n",
    "        return np.array(features[:13])\n",
    "    \n",
    "    def _payload_entropy(self, payload):\n",
    "        \"\"\"Calculate normalized Shannon entropy\"\"\"\n",
    "        if not payload or len(payload) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        byte_counts = np.bincount(list(payload), minlength=256)\n",
    "        probabilities = byte_counts[byte_counts > 0] / len(payload)\n",
    "        ent = entropy(probabilities, base=2) / 8.0\n",
    "        return min(ent, 1.0)\n",
    "    \n",
    "    def extract_flow_statistical_features(self, packets_array):\n",
    "        \"\"\"Extract 65 statistical features from packet sequence\"\"\"\n",
    "        stat_features = []\n",
    "        \n",
    "        # Ensure packets_array has the right shape\n",
    "        if len(packets_array.shape) != 2 or packets_array.shape[1] != 13:\n",
    "            # Fallback: return zero features\n",
    "            return np.zeros(65)\n",
    "        \n",
    "        for feat_idx in range(13):\n",
    "            feat_values = packets_array[:, feat_idx]\n",
    "            \n",
    "            # Handle edge cases\n",
    "            if len(feat_values) == 0:\n",
    "                stat_features.extend([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "            else:\n",
    "                stat_features.extend([\n",
    "                    np.mean(feat_values),\n",
    "                    np.std(feat_values) if len(feat_values) > 1 else 0.0,\n",
    "                    np.min(feat_values),\n",
    "                    np.max(feat_values),\n",
    "                    np.median(feat_values)\n",
    "                ])\n",
    "        \n",
    "        return np.array(stat_features)  # 65 features\n",
    "    \n",
    "    def process_pcap_for_ensemble(self, pcap_file, label, max_flows=500):\n",
    "        \"\"\"Extract both sequence and statistical features\"\"\"\n",
    "        print(f\"\\nProcessing {pcap_file}...\")\n",
    "        \n",
    "        flow_dict = defaultdict(list)\n",
    "        \n",
    "        try:\n",
    "            with PcapReader(pcap_file) as pcap:\n",
    "                for packet in tqdm(pcap, desc=\"   Reading packets\"):\n",
    "                    if IP not in packet:\n",
    "                        continue\n",
    "                    \n",
    "                    src_ip = packet[IP].src\n",
    "                    dst_ip = packet[IP].dst\n",
    "                    proto = packet[IP].proto\n",
    "                    \n",
    "                    if TCP in packet:\n",
    "                        src_port = packet[TCP].sport\n",
    "                        dst_port = packet[TCP].dport\n",
    "                    elif UDP in packet:\n",
    "                        src_port = packet[UDP].sport\n",
    "                        dst_port = packet[UDP].dport\n",
    "                    else:\n",
    "                        src_port = 0\n",
    "                        dst_port = 0\n",
    "                    \n",
    "                    flow_key = tuple(sorted([\n",
    "                        (src_ip, src_port, proto),\n",
    "                        (dst_ip, dst_port, proto)\n",
    "                    ]))\n",
    "                    \n",
    "                    features = self.extract_packet_features(packet)\n",
    "                    flow_dict[flow_key].append(features)\n",
    "            \n",
    "            print(f\"   Extracted {len(flow_dict)} flows\")\n",
    "            \n",
    "            # Create both sequence and statistical features\n",
    "            sequences = []\n",
    "            stat_features = []\n",
    "            labels = []\n",
    "            \n",
    "            # Debug: Check flow sizes\n",
    "            flow_sizes = [len(packets) for packets in flow_dict.values()]\n",
    "            if flow_sizes:\n",
    "                print(f\"   Flow size stats: min={min(flow_sizes)}, max={max(flow_sizes)}, \"\n",
    "                      f\"avg={np.mean(flow_sizes):.1f}, median={np.median(flow_sizes):.0f}\")\n",
    "                flows_with_3plus = sum(1 for s in flow_sizes if s >= 3)\n",
    "                print(f\"   Flows with â‰¥3 packets: {flows_with_3plus}/{len(flow_sizes)} ({flows_with_3plus/len(flow_sizes)*100:.1f}%)\")\n",
    "            \n",
    "            # REDUCED THRESHOLD: Use flows with at least 1 packet (will pad to sequence_length)\n",
    "            min_packets = 1  # Changed from 3 to 1\n",
    "            \n",
    "            for flow_key, packets in tqdm(list(flow_dict.items())[:max_flows], desc=\"   Creating features\"):\n",
    "                if len(packets) >= min_packets:\n",
    "                    # Pad/truncate for sequence\n",
    "                    if len(packets) < self.sequence_length:\n",
    "                        packets = packets + [np.zeros(13)] * (self.sequence_length - len(packets))\n",
    "                    else:\n",
    "                        packets = packets[:self.sequence_length]\n",
    "                    \n",
    "                    packets_array = np.array(packets)\n",
    "                    sequences.append(packets_array)\n",
    "                    stat_features.append(self.extract_flow_statistical_features(packets_array))\n",
    "                    labels.append(label)\n",
    "            \n",
    "            # Ensure we return proper shapes even if empty\n",
    "            if len(sequences) == 0:\n",
    "                print(f\"    Warning: No valid flows found (tried {len(flow_dict)} flows)\")\n",
    "                return (np.empty((0, self.sequence_length, 13)), \n",
    "                       np.empty((0, 65)), \n",
    "                       np.empty((0,)))\n",
    "            \n",
    "            print(f\"    Created {len(sequences)} valid sequences from {min(max_flows, len(flow_dict))} flows\")\n",
    "            return np.array(sequences), np.array(stat_features), np.array(labels)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return (np.empty((0, self.sequence_length, 13)), \n",
    "                   np.empty((0, 65)), \n",
    "                   np.empty((0,)))\n",
    "\n",
    "\n",
    "class EnsembleSteganographyDetector:\n",
    "    \"\"\"Ensemble model combining BiLSTM + ML classifiers\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=50, n_features=13):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_features = n_features\n",
    "        self.bilstm_model = None\n",
    "        self.rf_model = None\n",
    "        self.gb_model = None\n",
    "        self.lr_model = None\n",
    "        self.voting_model = None\n",
    "        self.scaler_seq = StandardScaler()\n",
    "        self.scaler_stat = StandardScaler()\n",
    "        self.history = None\n",
    "        \n",
    "    def build_bilstm_model(self):\n",
    "        \"\"\"Build BiLSTM model\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" BUILDING BiLSTM MODEL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        self.bilstm_model = models.Sequential([\n",
    "            layers.Input(shape=(self.sequence_length, self.n_features)),\n",
    "            layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.2)),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.2)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Bidirectional(layers.LSTM(32, return_sequences=False, dropout=0.2)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.1),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        self.bilstm_model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "        )\n",
    "        \n",
    "        print(\" BiLSTM model built\")\n",
    "    \n",
    "    def build_ml_models(self):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" BUILDING ML MODELS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        self.rf_model = RandomForestClassifier(\n",
    "            n_estimators=200, \n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.gb_model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.lr_model = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\" Random Forest model initialized\")\n",
    "        print(\" Gradient Boosting model initialized\")\n",
    "        print(\" Logistic Regression model initialized\")\n",
    "    \n",
    "    def train_ensemble(self, X_seq_train, X_stat_train, y_train,\n",
    "                      X_seq_val, X_stat_val, y_val,\n",
    "                      epochs=50, batch_size=32):\n",
    "        \"\"\"Train all models\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" TRAINING ENSEMBLE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Normalize sequence features\n",
    "        print(\"\\nNormalizing sequence features...\")\n",
    "        seq_shape = X_seq_train.shape\n",
    "        X_seq_train_flat = X_seq_train.reshape(-1, self.n_features)\n",
    "        X_seq_val_flat = X_seq_val.reshape(-1, self.n_features)\n",
    "        \n",
    "        X_seq_train_scaled = self.scaler_seq.fit_transform(X_seq_train_flat)\n",
    "        X_seq_val_scaled = self.scaler_seq.transform(X_seq_val_flat)\n",
    "        \n",
    "        X_seq_train = X_seq_train_scaled.reshape(seq_shape)\n",
    "        X_seq_val = X_seq_val_scaled.reshape(X_seq_val.shape)\n",
    "        \n",
    "        # Normalize statistical features\n",
    "        print(\"Normalizing statistical features...\")\n",
    "        X_stat_train = self.scaler_stat.fit_transform(X_stat_train)\n",
    "        X_stat_val = self.scaler_stat.transform(X_stat_val)\n",
    "        \n",
    "        # Train BiLSTM\n",
    "        print(\"\\n[1/4] Training BiLSTM...\")\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=8,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        self.history = self.bilstm_model.fit(\n",
    "            X_seq_train, y_train,\n",
    "            validation_data=(X_seq_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\" BiLSTM training complete\")\n",
    "        \n",
    "        # Train Random Forest\n",
    "        print(\"\\n[2/4] Training Random Forest...\")\n",
    "        self.rf_model.fit(X_stat_train, y_train)\n",
    "        rf_score = self.rf_model.score(X_stat_val, y_val)\n",
    "        print(f\" Random Forest validation accuracy: {rf_score:.4f}\")\n",
    "        \n",
    "        # Train Gradient Boosting\n",
    "        print(\"\\n[3/4] Training Gradient Boosting...\")\n",
    "        self.gb_model.fit(X_stat_train, y_train)\n",
    "        gb_score = self.gb_model.score(X_stat_val, y_val)\n",
    "        print(f\" Gradient Boosting validation accuracy: {gb_score:.4f}\")\n",
    "        \n",
    "        # Train Logistic Regression\n",
    "        print(\"\\n[4/4] Training Logistic Regression...\")\n",
    "        self.lr_model.fit(X_stat_train, y_train)\n",
    "        lr_score = self.lr_model.score(X_stat_val, y_val)\n",
    "        print(f\" Logistic Regression validation accuracy: {lr_score:.4f}\")\n",
    "        \n",
    "        return X_seq_train, X_stat_train, X_seq_val, X_stat_val\n",
    "    \n",
    "    def predict_ensemble(self, X_seq, X_stat, method='weighted_average'):\n",
    "        \"\"\"\n",
    "        Ensemble prediction methods:\n",
    "        - 'weighted_average': Weighted average of all predictions\n",
    "        - 'majority_vote': Majority voting\n",
    "        - 'stacking': Use predictions as features\n",
    "        \"\"\"\n",
    "        # Get individual predictions\n",
    "        bilstm_pred = self.bilstm_model.predict(X_seq, verbose=0).flatten()\n",
    "        rf_pred = self.rf_model.predict_proba(X_stat)[:, 1]\n",
    "        gb_pred = self.gb_model.predict_proba(X_stat)[:, 1]\n",
    "        lr_pred = self.lr_model.predict_proba(X_stat)[:, 1]\n",
    "        \n",
    "        if method == 'weighted_average':\n",
    "            # Weighted average (BiLSTM has highest weight)\n",
    "            weights = [0.4, 0.25, 0.25, 0.1]  # BiLSTM, RF, GB, LR\n",
    "            ensemble_pred = (\n",
    "                weights[0] * bilstm_pred +\n",
    "                weights[1] * rf_pred +\n",
    "                weights[2] * gb_pred +\n",
    "                weights[3] * lr_pred\n",
    "            )\n",
    "            \n",
    "        elif method == 'majority_vote':\n",
    "            # Convert to binary and vote\n",
    "            bilstm_binary = (bilstm_pred > 0.5).astype(int)\n",
    "            rf_binary = (rf_pred > 0.5).astype(int)\n",
    "            gb_binary = (gb_pred > 0.5).astype(int)\n",
    "            lr_binary = (lr_pred > 0.5).astype(int)\n",
    "            \n",
    "            votes = bilstm_binary + rf_binary + gb_binary + lr_binary\n",
    "            ensemble_pred = (votes >= 2).astype(float)\n",
    "            \n",
    "        else:  # max\n",
    "            ensemble_pred = np.maximum(np.maximum(bilstm_pred, rf_pred), \n",
    "                                      np.maximum(gb_pred, lr_pred))\n",
    "        \n",
    "        return ensemble_pred, {\n",
    "            'bilstm': bilstm_pred,\n",
    "            'rf': rf_pred,\n",
    "            'gb': gb_pred,\n",
    "            'lr': lr_pred\n",
    "        }\n",
    "    \n",
    "    def evaluate_ensemble(self, X_seq_test, X_stat_test, y_test, method='weighted_average'):\n",
    "        \"\"\"Evaluate ensemble performance\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" ENSEMBLE EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Normalize test data\n",
    "        X_seq_test_flat = X_seq_test.reshape(-1, self.n_features)\n",
    "        X_seq_test_scaled = self.scaler_seq.transform(X_seq_test_flat)\n",
    "        X_seq_test = X_seq_test_scaled.reshape(X_seq_test.shape)\n",
    "        X_stat_test = self.scaler_stat.transform(X_stat_test)\n",
    "        \n",
    "        # Get ensemble predictions\n",
    "        ensemble_pred_proba, individual_preds = self.predict_ensemble(\n",
    "            X_seq_test, X_stat_test, method=method\n",
    "        )\n",
    "        ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_test, ensemble_pred)\n",
    "        f1 = f1_score(y_test, ensemble_pred)\n",
    "        auc_score = roc_auc_score(y_test, ensemble_pred_proba)\n",
    "        \n",
    "        print(f\"\\n Ensemble Performance ({method}):\")\n",
    "        print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"   F1-Score:  {f1:.4f}\")\n",
    "        print(f\"   AUC-ROC:   {auc_score:.4f}\")\n",
    "        \n",
    "        # Individual model performance\n",
    "        print(f\"\\n Individual Model Performance:\")\n",
    "        for model_name, pred_proba in individual_preds.items():\n",
    "            pred = (pred_proba > 0.5).astype(int)\n",
    "            acc = accuracy_score(y_test, pred)\n",
    "            f1_ind = f1_score(y_test, pred)\n",
    "            print(f\"   {model_name.upper():<10} Acc: {acc:.4f}  F1: {f1_ind:.4f}\")\n",
    "        \n",
    "        print(f\"\\n Classification Report:\")\n",
    "        print(classification_report(y_test, ensemble_pred, \n",
    "                                   target_names=['Benign', 'Steganography']))\n",
    "        \n",
    "        cm = confusion_matrix(y_test, ensemble_pred)\n",
    "        print(f\"\\n Confusion Matrix:\")\n",
    "        print(f\"   True Negatives:  {cm[0,0]:>6,}\")\n",
    "        print(f\"   False Positives: {cm[0,1]:>6,}\")\n",
    "        print(f\"   False Negatives: {cm[1,0]:>6,}\")\n",
    "        print(f\"   True Positives:  {cm[1,1]:>6,}\")\n",
    "        \n",
    "        return ensemble_pred, ensemble_pred_proba, individual_preds, cm\n",
    "    \n",
    "    def plot_results(self, y_test, ensemble_pred, ensemble_pred_proba, \n",
    "                    individual_preds, cm, output_dir='results'):\n",
    "        \"\"\"Plot comprehensive results\"\"\"\n",
    "        print(\"\\n Generating visualizations...\")\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        \n",
    "        # 1. Confusion Matrix\n",
    "        ax1 = plt.subplot(2, 3, 1)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax1,\n",
    "                   xticklabels=['Benign', 'Stego'],\n",
    "                   yticklabels=['Benign', 'Stego'])\n",
    "        ax1.set_title('Ensemble Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 2. ROC Curves\n",
    "        ax2 = plt.subplot(2, 3, 2)\n",
    "        \n",
    "        # Ensemble ROC\n",
    "        fpr_ens, tpr_ens, _ = roc_curve(y_test, ensemble_pred_proba)\n",
    "        auc_ens = roc_auc_score(y_test, ensemble_pred_proba)\n",
    "        ax2.plot(fpr_ens, tpr_ens, label=f'Ensemble (AUC={auc_ens:.4f})', \n",
    "                linewidth=3, color='red')\n",
    "        \n",
    "        # Individual ROCs\n",
    "        colors = ['blue', 'green', 'orange', 'purple']\n",
    "        for (name, pred), color in zip(individual_preds.items(), colors):\n",
    "            fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "            auc_score = roc_auc_score(y_test, pred)\n",
    "            ax2.plot(fpr, tpr, label=f'{name.upper()} (AUC={auc_score:.4f})',\n",
    "                    linewidth=2, alpha=0.7, color=color)\n",
    "        \n",
    "        ax2.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "        ax2.set_xlabel('False Positive Rate')\n",
    "        ax2.set_ylabel('True Positive Rate')\n",
    "        ax2.set_title('ROC Curves Comparison', fontsize=12, fontweight='bold')\n",
    "        ax2.legend(fontsize=8)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Model Accuracy Comparison\n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        model_names = ['BiLSTM', 'RF', 'GB', 'LR', 'Ensemble']\n",
    "        accuracies = []\n",
    "        for name, pred in individual_preds.items():\n",
    "            accuracies.append(accuracy_score(y_test, (pred > 0.5).astype(int)))\n",
    "        accuracies.append(accuracy_score(y_test, ensemble_pred))\n",
    "        \n",
    "        bars = ax3.bar(model_names, accuracies, \n",
    "                      color=['#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#e74c3c'])\n",
    "        ax3.set_ylabel('Accuracy')\n",
    "        ax3.set_title('Model Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "        ax3.set_ylim([0.9, 1.0])\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 4. Prediction Distribution\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        ax4.hist(ensemble_pred_proba[y_test == 0], bins=30, alpha=0.7, \n",
    "                label='Benign', color='green', edgecolor='black')\n",
    "        ax4.hist(ensemble_pred_proba[y_test == 1], bins=30, alpha=0.7, \n",
    "                label='Stego', color='red', edgecolor='black')\n",
    "        ax4.axvline(0.5, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "        ax4.set_xlabel('Prediction Probability')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.set_title('Ensemble Prediction Distribution', fontsize=12, fontweight='bold')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. BiLSTM Training History\n",
    "        if self.history:\n",
    "            ax5 = plt.subplot(2, 3, 5)\n",
    "            ax5.plot(self.history.history['accuracy'], label='Train', linewidth=2)\n",
    "            ax5.plot(self.history.history['val_accuracy'], label='Val', linewidth=2)\n",
    "            ax5.set_xlabel('Epoch')\n",
    "            ax5.set_ylabel('Accuracy')\n",
    "            ax5.set_title('BiLSTM Training History', fontsize=12, fontweight='bold')\n",
    "            ax5.legend()\n",
    "            ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. F1-Score Comparison\n",
    "        ax6 = plt.subplot(2, 3, 6)\n",
    "        f1_scores = []\n",
    "        for name, pred in individual_preds.items():\n",
    "            f1_scores.append(f1_score(y_test, (pred > 0.5).astype(int)))\n",
    "        f1_scores.append(f1_score(y_test, ensemble_pred))\n",
    "        \n",
    "        bars = ax6.bar(model_names, f1_scores,\n",
    "                      color=['#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#e74c3c'])\n",
    "        ax6.set_ylabel('F1-Score')\n",
    "        ax6.set_title('Model F1-Score Comparison', fontsize=12, fontweight='bold')\n",
    "        ax6.set_ylim([0.9, 1.0])\n",
    "        ax6.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        for bar, f1 in zip(bars, f1_scores):\n",
    "            height = bar.get_height()\n",
    "            ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{f1:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_path = os.path.join(output_dir, 'ensemble_results.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\" Saved: {plot_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def save_models(self, output_dir='models'):\n",
    "        \"\"\"Save all trained models\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" SAVING ENSEMBLE MODELS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Save BiLSTM\n",
    "        bilstm_path = os.path.join(output_dir, f'ensemble_bilstm_{timestamp}.h5')\n",
    "        self.bilstm_model.save(bilstm_path)\n",
    "        print(f\"\\nâœ“ BiLSTM saved: {bilstm_path}\")\n",
    "        \n",
    "        # Save ML models\n",
    "        ensemble_dict = {\n",
    "            'rf': self.rf_model,\n",
    "            'gb': self.gb_model,\n",
    "            'lr': self.lr_model,\n",
    "            'scaler_seq': self.scaler_seq,\n",
    "            'scaler_stat': self.scaler_stat,\n",
    "            'metadata': {\n",
    "                'timestamp': timestamp,\n",
    "                'sequence_length': self.sequence_length,\n",
    "                'n_features': self.n_features\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        ensemble_path = os.path.join(output_dir, f'ensemble_models_{timestamp}.pkl')\n",
    "        with open(ensemble_path, 'wb') as f:\n",
    "            pickle.dump(ensemble_dict, f)\n",
    "        print(f\" ML models saved: {ensemble_path}\")\n",
    "        \n",
    "        print(f\"\\n All models saved in {output_dir}/\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ensemble training pipeline\"\"\"\n",
    "    \n",
    "    benign_pcap = \"Dataset/Benign_Dump.pcap\"\n",
    "    stego_pcap = \"Dataset/steganography_dataset_20251016_233034.pcap\"\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(benign_pcap):\n",
    "        print(f\" Benign PCAP not found: {benign_pcap}\")\n",
    "        return\n",
    "    if not os.path.exists(stego_pcap):\n",
    "        print(f\" Stego PCAP not found: {stego_pcap}\")\n",
    "        return\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" EXTRACTING FEATURES FROM PCAPS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    extractor = EnsembleFeatureExtractor(sequence_length=50)\n",
    "    \n",
    "    # Process both PCAPs\n",
    "    X_seq_benign, X_stat_benign, y_benign = extractor.process_pcap_for_ensemble(\n",
    "        benign_pcap, label=0, max_flows=500\n",
    "    )\n",
    "    \n",
    "    X_seq_stego, X_stat_stego, y_stego = extractor.process_pcap_for_ensemble(\n",
    "        stego_pcap, label=1, max_flows=500\n",
    "    )\n",
    "    \n",
    "    # Validate data\n",
    "    if len(X_seq_benign) == 0 or len(X_seq_stego) == 0:\n",
    "        print(\"\\n Error: One or both datasets are empty!\")\n",
    "        print(f\"   Benign flows: {len(X_seq_benign)}\")\n",
    "        print(f\"   Stego flows: {len(X_seq_stego)}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n Feature Extraction Complete:\")\n",
    "    print(f\"   Benign: {len(X_seq_benign)} flows\")\n",
    "    print(f\"   Stego:  {len(X_seq_stego)} flows\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    X_seq = np.concatenate([X_seq_benign, X_seq_stego], axis=0)\n",
    "    X_stat = np.concatenate([X_stat_benign, X_stat_stego], axis=0)\n",
    "    y = np.concatenate([y_benign, y_stego], axis=0)\n",
    "    \n",
    "    print(f\"\\n Combined Dataset:\")\n",
    "    print(f\"   Sequences shape: {X_seq.shape}\")\n",
    "    print(f\"   Statistical shape: {X_stat.shape}\")\n",
    "    print(f\"   Labels shape: {y.shape}\")\n",
    "    print(f\"   Total samples: {len(y)}\")\n",
    "    \n",
    "    # Check minimum samples\n",
    "    if len(y) < 100:\n",
    "        print(f\"\\n Warning: Only {len(y)} samples. Need at least 100 for reliable training.\")\n",
    "        print(\"   Consider increasing max_flows or using more PCAP files.\")\n",
    "        return\n",
    "    \n",
    "    # Split data\n",
    "    print(f\"\\n Splitting data...\")\n",
    "    indices = np.arange(len(y))\n",
    "    train_idx, test_idx = train_test_split(indices, test_size=0.2, \n",
    "                                          stratify=y, random_state=42)\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=0.2,\n",
    "                                         stratify=y[train_idx], random_state=42)\n",
    "    \n",
    "    X_seq_train, X_seq_val, X_seq_test = X_seq[train_idx], X_seq[val_idx], X_seq[test_idx]\n",
    "    X_stat_train, X_stat_val, X_stat_test = X_stat[train_idx], X_stat[val_idx], X_stat[test_idx]\n",
    "    y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
    "    \n",
    "    print(f\" Data Split:\")\n",
    "    print(f\"   Train: {len(train_idx)} samples\")\n",
    "    print(f\"   Val:   {len(val_idx)} samples\")\n",
    "    print(f\"   Test:  {len(test_idx)} samples\")\n",
    "    \n",
    "    # Initialize ensemble\n",
    "    ensemble = EnsembleSteganographyDetector(sequence_length=50, n_features=13)\n",
    "    \n",
    "    # Build models\n",
    "    ensemble.build_bilstm_model()\n",
    "    ensemble.build_ml_models()\n",
    "    \n",
    "    # Train ensemble\n",
    "    try:\n",
    "        X_seq_train, X_stat_train, X_seq_val, X_stat_val = ensemble.train_ensemble(\n",
    "            X_seq_train, X_stat_train, y_train,\n",
    "            X_seq_val, X_stat_val, y_val,\n",
    "            epochs=10, batch_size=32\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "    # Evaluate\n",
    "    try:\n",
    "        ensemble_pred, ensemble_pred_proba, individual_preds, cm = ensemble.evaluate_ensemble(\n",
    "            X_seq_test, X_stat_test, y_test, method='weighted_average'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "    # Plot results\n",
    "    try:\n",
    "        ensemble.plot_results(y_test, ensemble_pred, ensemble_pred_proba,\n",
    "                             individual_preds, cm)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Plotting failed: {e}\")\n",
    "        print(\"   Continuing without plots...\")\n",
    "    \n",
    "    # Save models\n",
    "    try:\n",
    "        ensemble.save_models()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Model saving failed: {e}\")\n",
    "        print(\"   Models were trained but not saved.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  ENSEMBLE TRAINING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n Check ./results/ for visualizations\")\n",
    "    print(\" Check ./models/ for saved models\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9968ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
